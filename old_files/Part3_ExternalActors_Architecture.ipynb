{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "78kb73ba51Q4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6GKbiFh7Mwb",
        "outputId": "15b0ab6a-8630-430a-88a0-930428ebe13f"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE3VfXUW51Q7",
        "outputId": "66ef486c-3057-4779-b49e-deb5ee14c4da"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "data_path = '/content/drive/MyDrive/CS7643Group/Dataset/training/parsed_data_final_srishti.pkl'\n",
        "with open(data_path, \"rb\") as file:\n",
        "  data = pickle.load(file)\n",
        "\n",
        "print(data.shape)\n",
        "\n",
        "data = data[:, :-1, :]\n",
        "data = data[:, :, 1:]\n",
        "# new_data = data[:, :, 10:]\n",
        "# data = np.concatenate((another_data, new_data), axis = 2)\n",
        "print(f\"Shape of full dataset: {data.shape}\")\n",
        "# print(data[0])\n",
        "X_train = data[:int(0.8*len(data)), :80, :]\n",
        "X_val = data[int(0.8*len(data)): , :80, :]\n",
        "\n",
        "y_train = data[:int(0.8*len(data)), 80:, :]\n",
        "y_val = data[int(0.8*len(data)): , 80:, :]\n",
        "\n",
        "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIJvcWBt46Xr",
        "outputId": "2e33703c-2772-4ff4-91af-f298aa0a1fde"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "data_path = '/content/drive/MyDrive/CS7643Group/Dataset/parsed_data_no_timestamps.pkl'\n",
        "with open(data_path, \"rb\") as file:\n",
        "  data = pickle.load(file)\n",
        "print(f\"Shape of full dataset: {data.shape}\")\n",
        "\n",
        "print(data[0])\n",
        "\n",
        "X_train = data[:int(0.8*len(data)), :80, :]\n",
        "X_val = data[int(0.8*len(data)): , :80, :]\n",
        "\n",
        "y_train = data[:int(0.8*len(data)), 80:, :]\n",
        "y_val = data[int(0.8*len(data)): , 80:, :]\n",
        "\n",
        "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGxoNAjP-BAJ"
      },
      "source": [
        "# Initial External Actors Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "9Slm3WzA51Q8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TrajectoryLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1):  # Set default n to 10\n",
        "        super(TrajectoryLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_features * output_timesteps)\n",
        "        self.output_features = output_features\n",
        "        self.output_timesteps = output_timesteps\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_length, input_size)\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        # hn: (num_layers, batch_size, hidden_size)\n",
        "        # want to take the last layer so that you are left with (batch_size, hidden_size)\n",
        "        out = self.fc(hn[-1])\n",
        "        return out.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, 10, input_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "-emFzp-M51Q9"
      },
      "outputs": [],
      "source": [
        "class TrajectoryLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TrajectoryLoss, self).__init__()\n",
        "        self.max_acceleration = 3.0\n",
        "\n",
        "    def forward(self, predictions, targets, last_input_state, dt=0.1):\n",
        "        # Reconstruct trajectory from velocities\n",
        "        pred_velocity = predictions[:, :, :]\n",
        "        cum_disp = torch.cumsum(pred_velocity * dt, dim=1)\n",
        "        # print(pred_velocity.shape)\n",
        "        # (32, 10, 2)\n",
        "        pred_position = cum_disp + last_input_state.unsqueeze(1)\n",
        "        # print(pred_position.shape)\n",
        "\n",
        "        target_position = targets[:, :, :2]\n",
        "        target_velocity = targets[:, :, 2:4]\n",
        "\n",
        "        # Calculate MSE for position\n",
        "\n",
        "        position_loss = F.mse_loss(pred_position, target_position)\n",
        "        velocity_loss = F.mse_loss(pred_velocity, target_velocity)\n",
        "        terminal_position_loss = F.mse_loss(pred_position[:, -1, :], target_position[:, -1, :]) + F.mse_loss(pred_position[:, 0, :], target_position[:, 0, :])\n",
        "        smoothness_loss = self.smoothness_loss(pred_position)\n",
        "\n",
        "        total_loss = position_loss + velocity_loss + smoothness_loss + 2 * terminal_position_loss\n",
        "        return total_loss\n",
        "\n",
        "    def acceleration_limit_loss(self, pred_velocity, dt=0.1):\n",
        "        # Ensure accelerations remain within feasible limits by penalizing large changes in velocity\n",
        "        approx_acceleration = (pred_velocity[:, :-1, :] - pred_velocity[:, 1:, :]) / dt  # Difference between consecutive velocities\n",
        "        acceleration_norm = torch.norm(approx_acceleration, dim=2)\n",
        "        excess_acceleration = torch.clamp(acceleration_norm - self.max_acceleration, min=0.0)\n",
        "        return torch.mean(excess_acceleration ** 2)\n",
        "\n",
        "\n",
        "    def smoothness_loss(self, pred_position):\n",
        "        # Calculate the difference between consecutive positions\n",
        "        diff = pred_position[:, :-1, :] - pred_position[:, 1:, :]\n",
        "        smoothness = torch.norm(diff, dim=2)\n",
        "        return torch.mean(smoothness)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-Cz-wsV51Q9",
        "outputId": "eea6a837-9363-4b01-f45a-64f5060c555c"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, initial_lr=0.001, T_max=10):\n",
        "    criterion = TrajectoryLoss()  # Use the custom loss function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)  # Adam optimizer\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        epoch_train_loss = 0  # Initialize training loss for the epoch\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            outputs = model(batch_x)  # Forward pass\n",
        "            loss = criterion(outputs, batch_y, batch_x[:, 0, :2])  # Compute loss using the custom loss function\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            epoch_train_loss += loss.item()  # Accumulate training loss\n",
        "\n",
        "        # Update learning rate based on the cosine decay schedule\n",
        "        scheduler.step()\n",
        "\n",
        "        # Average training loss for the epoch\n",
        "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)  # Store training loss\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        epoch_val_loss = 0  # Initialize validation loss for the epoch\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "                outputs = model(batch_x)\n",
        "                loss = criterion(outputs, batch_y, batch_x[:, 0, :2])\n",
        "                epoch_val_loss += loss.item()  # Accumulate validation loss\n",
        "\n",
        "            # Average validation loss for the epoch\n",
        "            avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "            val_losses.append(avg_val_loss)  # Store validation loss\n",
        "            print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "# Create Tensor datasets\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_train.shape[2]  # Number of features\n",
        "# hidden_size = 64  # You can adjust this\n",
        "\n",
        "hidden_size = 32  # You can adjust this\n",
        "\n",
        "# input_size = 5\n",
        "# model = TrajectoryLSTM(input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=4).to(\"cuda\")\n",
        "\n",
        "model = TrajectoryLSTM(input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1).to(\"cuda\")\n",
        "\n",
        "\n",
        "# Train the model with cosine decay learning rate\n",
        "\n",
        "\n",
        "train_model(model, train_loader, val_loader, num_epochs=150, initial_lr=1e-2, T_max=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmg9hEag51Q_",
        "outputId": "4c14034c-a36a-4b7c-c694-dd26f77000de"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor(X_val, dtype=torch.float32).to(\"cuda\")\n",
        "y = torch.tensor(y_val, dtype=torch.float32).to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(x)\n",
        "    cum_disp = torch.cumsum(outputs * 0.1, dim=1)\n",
        "    pred_position = cum_disp + y[:, 0, :2].unsqueeze(1)\n",
        "\n",
        "    distances = torch.norm(pred_position - y[:, :, :2], dim=2)\n",
        "    mean_ade = distances.mean().item()\n",
        "#LSTM\n",
        "print(\"Mean ADE:\", mean_ade)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "AW2cpEG_51Q_",
        "outputId": "4a555a3c-030a-4ed3-d82d-b440675ecd00"
      },
      "outputs": [],
      "source": [
        "# Choose a random example from the validation dataset\n",
        "example_index = 407 # np.random.randint(0, len(X_val))\n",
        "# 300 produces no movement for x and y\n",
        "# 250 produces an erratic motion for x and y (curvy)\n",
        "\n",
        "past_traj = X_val[example_index]\n",
        "future_traj = y_val[example_index]\n",
        "\n",
        "# Get the predicted future trajectory\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  X = torch.tensor(past_traj[np.newaxis, :, :], dtype=torch.float32).to(\"cuda\")\n",
        "  predicted_vels = model(X)\n",
        "\n",
        "  predicted_vels = predicted_vels[0]\n",
        "  cum_disp = torch.cumsum(predicted_vels * 0.1, dim=1)\n",
        "  future_pred = cum_disp + X[0, -1, :2]\n",
        "  future_pred = future_pred.cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "# Plot the past trajectory, actual future trajectory, and predicted future trajectory\n",
        "#LSTM\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(past_traj[:, 0], past_traj[:, 1], label='Past Trajectory', color='blue')\n",
        "plt.plot(future_traj[:, 0], future_traj[:, 1], label='Actual Future Trajectory', color='red')\n",
        "plt.plot(future_pred[:, 0], future_pred[:, 1], label='Predicted Future Trajectory', color='green')\n",
        "plt.xlabel('X Coordinate')\n",
        "plt.ylabel('Y Coordinate')\n",
        "plt.title(f\"Predicted vs. Actual Future Trajectory (Example {example_index})\")\n",
        "plt.axis('equal')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# print(past_traj[:, :2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGZsDNhgifRK"
      },
      "source": [
        "# Bi-directional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N_UBc0zkigRI",
        "outputId": "04ea1eee-8e45-4191-acbf-db8e59b3a337"
      },
      "outputs": [],
      "source": [
        "#Kavya and Srishti\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TrajectoryLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1):  # Set default n to 10\n",
        "        super(TrajectoryLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_size * 2, output_features * output_timesteps)\n",
        "        self.output_features = output_features\n",
        "        self.output_timesteps = output_timesteps\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_length, input_size)\n",
        "        # _, (hn, _) = self.lstm(x)\n",
        "        # # hn: (num_layers, batch_size, hidden_size)\n",
        "        # # want to take the last layer so that you are left with (batch_size, hidden_size)\n",
        "        # out = self.fc(hn[-1])\n",
        "        # return out.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, 10, input_size)\n",
        "\n",
        "\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        # hn: (num_layers * num_directions, batch_size, hidden_size)\n",
        "        hn = hn.view(self.lstm.num_layers, 2, x.size(0), self.lstm.hidden_size)  # Reshape to access both directions\n",
        "        hn = torch.cat((hn[-1, 0], hn[-1, 1]), dim=-1)  # Concatenate forward and backward hidden states\n",
        "        out = self.fc(hn)\n",
        "        return out.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, output_timesteps, output_features)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TrajectoryLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TrajectoryLoss, self).__init__()\n",
        "        self.max_acceleration = 3.0\n",
        "\n",
        "    def forward(self, predictions, targets, last_input_state, dt=0.1):\n",
        "        # Reconstruct trajectory from velocities\n",
        "        pred_velocity = predictions[:, :, :]\n",
        "        cum_disp = torch.cumsum(pred_velocity * dt, dim=1)\n",
        "        # print(pred_velocity.shape)\n",
        "        # (32, 10, 2)\n",
        "\n",
        "        pred_position = cum_disp + last_input_state.unsqueeze(1)\n",
        "\n",
        "        # print(pred_position.shape)\n",
        "\n",
        "        target_position = targets[:, :, :2]\n",
        "        target_velocity = targets[:, :, 2:4]\n",
        "\n",
        "        # Calculate MSE for position\n",
        "\n",
        "        position_loss = F.mse_loss(pred_position, target_position)\n",
        "\n",
        "        velocity_loss = F.mse_loss(pred_velocity, target_velocity)\n",
        "        terminal_position_loss = F.mse_loss(pred_position[:, -1, :], target_position[:, -1, :]) + F.mse_loss(pred_position[:, 0, :], target_position[:, 0, :])\n",
        "        smoothness_loss = self.smoothness_loss(pred_position)\n",
        "\n",
        "        total_loss = position_loss + velocity_loss + smoothness_loss + 2 * terminal_position_loss\n",
        "        return total_loss\n",
        "\n",
        "    def acceleration_limit_loss(self, pred_velocity, dt=0.1):\n",
        "        # Ensure accelerations remain within feasible limits by penalizing large changes in velocity\n",
        "        approx_acceleration = (pred_velocity[:, :-1, :] - pred_velocity[:, 1:, :]) / dt  # Difference between consecutive velocities\n",
        "        acceleration_norm = torch.norm(approx_acceleration, dim=2)\n",
        "        excess_acceleration = torch.clamp(acceleration_norm - self.max_acceleration, min=0.0)\n",
        "        return torch.mean(excess_acceleration ** 2)\n",
        "\n",
        "\n",
        "    def smoothness_loss(self, pred_position):\n",
        "        # Calculate the difference between consecutive positions\n",
        "        diff = pred_position[:, :-1, :] - pred_position[:, 1:, :]\n",
        "        smoothness = torch.norm(diff, dim=2)\n",
        "        return torch.mean(smoothness)\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, initial_lr=0.001, T_max=10):\n",
        "    criterion = TrajectoryLoss()  # Use the custom loss function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)  # Adam optimizer\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        epoch_train_loss = 0  # Initialize training loss for the epoch\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            outputs = model(batch_x)  # Forward pass\n",
        "            loss = criterion(outputs, batch_y, batch_x[:, 0, :2])  # Compute loss using the custom loss function\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            epoch_train_loss += loss.item()  # Accumulate training loss\n",
        "\n",
        "        # Update learning rate based on the cosine decay schedule\n",
        "        scheduler.step()\n",
        "\n",
        "        # Average training loss for the epoch\n",
        "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)  # Store training loss\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        epoch_val_loss = 0  # Initialize validation loss for the epoch\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "                outputs = model(batch_x)\n",
        "                loss = criterion(outputs, batch_y, batch_x[:, 0, :2])\n",
        "                epoch_val_loss += loss.item()  # Accumulate validation loss\n",
        "\n",
        "            # Average validation loss for the epoch\n",
        "            avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "            val_losses.append(avg_val_loss)  # Store validation loss\n",
        "            print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "# Create Tensor datasets\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_train.shape[2]  # Number of features\n",
        "hidden_size = 64  # You can adjust this\n",
        "# input_size = 5\n",
        "model = TrajectoryLSTM(input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=4).to(\"cuda\")\n",
        "\n",
        "# Train the model with cosine decay learning rate\n",
        "train_model(model, train_loader, val_loader, num_epochs=100, initial_lr=1e-2, T_max=50)\n",
        "\n",
        "\n",
        "x = torch.tensor(X_val, dtype=torch.float32).to(\"cuda\")\n",
        "y = torch.tensor(y_val, dtype=torch.float32).to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(x)\n",
        "    cum_disp = torch.cumsum(outputs * 0.1, dim=1)\n",
        "    pred_position = cum_disp + y[:, 0, :2].unsqueeze(1)\n",
        "\n",
        "    distances = torch.norm(pred_position - y[:, :, :2], dim=2)\n",
        "    mean_ade = distances.mean().item()\n",
        "#LSTM\n",
        "print(\"Mean ADE:\", mean_ade)\n",
        "\n",
        "# Choose a random example from the validation dataset\n",
        "example_index = 400 # np.random.randint(0, len(X_val))\n",
        "past_traj = X_val[example_index]\n",
        "future_traj = y_val[example_index]\n",
        "\n",
        "# Get the predicted future trajectory\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  X = torch.tensor(past_traj[np.newaxis, :, :], dtype=torch.float32).to(\"cuda\")\n",
        "  predicted_vels = model(X)\n",
        "\n",
        "  predicted_vels = predicted_vels[0]\n",
        "  cum_disp = torch.cumsum(predicted_vels * 0.1, dim=1)\n",
        "  future_pred = cum_disp + X[0, -1, :2]\n",
        "  future_pred = future_pred.cpu().numpy()\n",
        "\n",
        "# print(past_traj[:, :2])\n",
        "\n",
        "\n",
        "# Plot the past trajectory, actual future trajectory, and predicted future trajectory\n",
        "#LSTM\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(past_traj[:, 0], past_traj[:, 1], label='Past Trajectory', color='blue')\n",
        "plt.plot(future_traj[:, 0], future_traj[:, 1], label='Actual Future Trajectory', color='red')\n",
        "plt.plot(future_pred[:, 0], future_pred[:, 1], label='Predicted Future Trajectory', color='green')\n",
        "plt.xlabel('X Coordinate')\n",
        "plt.ylabel('Y Coordinate')\n",
        "plt.title(f\"Predicted vs. Actual Future Trajectory (Example {example_index})\")\n",
        "plt.axis('equal')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0SPF0oHv4D4"
      },
      "source": [
        "# Additional Layers - eg. Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PhNwbDbn9S9O",
        "outputId": "17a536f4-167c-4ca4-a06a-413e5128207f"
      },
      "outputs": [],
      "source": [
        "#Kavya and Srishti\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TrajectoryLSTM(nn.Module):\n",
        "    # def __init__(self, input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1, dropout=0.2):\n",
        "    #     super(TrajectoryLSTM, self).__init__()\n",
        "    #     self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "    #     self.dropout = nn.Dropout(dropout)  # Dropout layer\n",
        "    #     self.fc = nn.Linear(hidden_size * 2, output_features * output_timesteps)\n",
        "    #     self.output_features = output_features\n",
        "    #     self.output_timesteps = output_timesteps\n",
        "\n",
        "    # def forward(self, x):\n",
        "    #     # x: (batch_size, seq_length, input_size)\n",
        "    #     # _, (hn, _) = self.lstm(x)\n",
        "    #     # # hn: (num_layers * num_directions, batch_size, hidden_size)\n",
        "    #     # hn = hn.view(self.lstm.num_layers, 2, x.size(0), self.lstm.hidden_size)  # Reshape to access both directions\n",
        "    #     # hn = torch.cat((hn[-1, 0], hn[-1, 1]), dim=-1)  # Concatenate forward and backward hidden states\n",
        "    #     # hn = self.dropout(hn)  # Apply dropout\n",
        "    #     # out = self.fc(hn)\n",
        "    #     # return out.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, output_timesteps, output_features)\n",
        "\n",
        "    #     _, (hn, _) = self.lstm(x)\n",
        "    #     # hn: (num_layers, batch_size, hidden_size)\n",
        "    #     # want to take the last layer so that you are left with (batch_size, hidden_size)\n",
        "    #     out = self.fc(hn[-1])\n",
        "    #     return out.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, 10, input_size)\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1, dropout=0.2):  # Set default n to 10\n",
        "        super(TrajectoryLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)  # Dropout layer\n",
        "        # self.fc = nn.Linear(hidden_size, output_features * output_timesteps)\n",
        "        self.fc = nn.Sequential(\n",
        "          nn.Linear(hidden_size, 64),\n",
        "          nn.BatchNorm1d(64),\n",
        "          nn.Dropout(0.5), # Dropout after the first FC layer\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(64, 32),\n",
        "          nn.BatchNorm1d(32),\n",
        "          nn.Dropout(0.3), # Dropout after the second FC layer\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(32, output_features * output_timesteps))\n",
        "        self.output_features = output_features\n",
        "        self.output_timesteps = output_timesteps\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_length, input_size)\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        # hn: (num_layers, batch_size, hidden_size)\n",
        "        # want to take the last layer so that you are left with (batch_size, hidden_size)\n",
        "        hn = self.dropout(hn)\n",
        "        out = self.fc(hn[-1])\n",
        "        return out.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, 10, input_size)\n",
        "\n",
        "\n",
        "\n",
        "class TrajectoryLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TrajectoryLoss, self).__init__()\n",
        "        self.max_acceleration = 3.0\n",
        "\n",
        "    def forward(self, predictions, targets, last_input_state, dt=0.1):\n",
        "        # Reconstruct trajectory from velocities\n",
        "        pred_velocity = predictions[:, :, :]\n",
        "        cum_disp = torch.cumsum(pred_velocity * dt, dim=1)\n",
        "        # print(pred_velocity.shape)\n",
        "        # (32, 10, 2)\n",
        "\n",
        "        pred_position = cum_disp + last_input_state.unsqueeze(1)\n",
        "\n",
        "        # print(pred_position.shape)\n",
        "\n",
        "        target_position = targets[:, :, :2]\n",
        "        target_velocity = targets[:, :, 2:4]\n",
        "\n",
        "        # Calculate MSE for position\n",
        "\n",
        "        position_loss = F.mse_loss(pred_position, target_position)\n",
        "\n",
        "        velocity_loss = F.mse_loss(pred_velocity, target_velocity)\n",
        "        terminal_position_loss = F.mse_loss(pred_position[:, -1, :], target_position[:, -1, :]) + F.mse_loss(pred_position[:, 0, :], target_position[:, 0, :])\n",
        "        smoothness_loss = self.smoothness_loss(pred_position)\n",
        "\n",
        "        total_loss = position_loss + velocity_loss + smoothness_loss + 2 * terminal_position_loss\n",
        "        return total_loss\n",
        "\n",
        "    def acceleration_limit_loss(self, pred_velocity, dt=0.1):\n",
        "        # Ensure accelerations remain within feasible limits by penalizing large changes in velocity\n",
        "        approx_acceleration = (pred_velocity[:, :-1, :] - pred_velocity[:, 1:, :]) / dt  # Difference between consecutive velocities\n",
        "        acceleration_norm = torch.norm(approx_acceleration, dim=2)\n",
        "        excess_acceleration = torch.clamp(acceleration_norm - self.max_acceleration, min=0.0)\n",
        "        return torch.mean(excess_acceleration ** 2)\n",
        "\n",
        "\n",
        "    def smoothness_loss(self, pred_position):\n",
        "        # Calculate the difference between consecutive positions\n",
        "        diff = pred_position[:, :-1, :] - pred_position[:, 1:, :]\n",
        "        smoothness = torch.norm(diff, dim=2)\n",
        "        return torch.mean(smoothness)\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, initial_lr=0.001, T_max=10):\n",
        "    criterion = TrajectoryLoss()  # Use the custom loss function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)  # Adam optimizer\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        epoch_train_loss = 0  # Initialize training loss for the epoch\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            outputs = model(batch_x)  # Forward pass\n",
        "            loss = criterion(outputs, batch_y, batch_x[:, 0, :2])  # Compute loss using the custom loss function\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            epoch_train_loss += loss.item()  # Accumulate training loss\n",
        "\n",
        "        # Update learning rate based on the cosine decay schedule\n",
        "        scheduler.step()\n",
        "\n",
        "        # Average training loss for the epoch\n",
        "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)  # Store training loss\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        epoch_val_loss = 0  # Initialize validation loss for the epoch\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "                outputs = model(batch_x)\n",
        "                loss = criterion(outputs, batch_y, batch_x[:, 0, :2])\n",
        "                epoch_val_loss += loss.item()  # Accumulate validation loss\n",
        "\n",
        "            # Average validation loss for the epoch\n",
        "            avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "            val_losses.append(avg_val_loss)  # Store validation loss\n",
        "            print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "# Create Tensor datasets\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_train.shape[2]  # Number of features\n",
        "hidden_size = 64  # You can adjust this\n",
        "# input_size = 5\n",
        "model = TrajectoryLSTM(input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1, dropout=0.5).to(\"cuda\")\n",
        "\n",
        "# Train the model with cosine decay learning rate\n",
        "train_model(model, train_loader, val_loader, num_epochs=150, initial_lr=1e-2, T_max=50)\n",
        "\n",
        "def plot_loss_curves(train_losses, val_losses):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(train_losses, label='Training Loss', color='blue')\n",
        "    plt.plot(val_losses, label='Validation Loss', color='orange')\n",
        "    plt.title('Training and Validation Loss Curves')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_curves(train_losses, val_losses)\n",
        "\n",
        "x = torch.tensor(X_val, dtype=torch.float32).to(\"cuda\")\n",
        "y = torch.tensor(y_val, dtype=torch.float32).to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(x)\n",
        "    cum_disp = torch.cumsum(outputs * 0.1, dim=1)\n",
        "    pred_position = cum_disp + y[:, 0, :2].unsqueeze(1)\n",
        "\n",
        "    distances = torch.norm(pred_position - y[:, :, :2], dim=2)\n",
        "    mean_ade = distances.mean().item()\n",
        "#LSTM\n",
        "print(\"Mean ADE:\", mean_ade)\n",
        "\n",
        "# Choose a random example from the validation dataset\n",
        "example_index = 400 # np.random.randint(0, len(X_val))\n",
        "past_traj = X_val[example_index]\n",
        "future_traj = y_val[example_index]\n",
        "\n",
        "# Get the predicted future trajectory\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  X = torch.tensor(past_traj[np.newaxis, :, :], dtype=torch.float32).to(\"cuda\")\n",
        "  predicted_vels = model(X)\n",
        "\n",
        "  predicted_vels = predicted_vels[0]\n",
        "  cum_disp = torch.cumsum(predicted_vels * 0.1, dim=1)\n",
        "  future_pred = cum_disp + X[0, -1, :2]\n",
        "  future_pred = future_pred.cpu().numpy()\n",
        "\n",
        "# print(past_traj[:, :2])\n",
        "\n",
        "\n",
        "# Plot the past trajectory, actual future trajectory, and predicted future trajectory\n",
        "#LSTM\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(past_traj[:, 0], past_traj[:, 1], label='Past Trajectory', color='blue')\n",
        "plt.plot(future_traj[:, 0], future_traj[:, 1], label='Actual Future Trajectory', color='red')\n",
        "plt.plot(future_pred[:, 0], future_pred[:, 1], label='Predicted Future Trajectory', color='green')\n",
        "plt.xlabel('X Coordinate')\n",
        "plt.ylabel('Y Coordinate')\n",
        "plt.title(f\"Predicted vs. Actual Future Trajectory (Example {example_index})\")\n",
        "plt.axis('equal')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZx1H4khGUp1"
      },
      "source": [
        "# Attention Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AngW2UWRGZZf",
        "outputId": "0c0002a3-3000-4a75-8ece-097b42b47ed0"
      },
      "outputs": [],
      "source": [
        "#Kavya and Srishti\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# class TrajectoryLSTM(nn.Module):\n",
        "#     # def __init__(self, input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1, dropout=0.2):\n",
        "#     #     super(TrajectoryLSTM, self).__init__()\n",
        "#     #     self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "#     #     self.dropout = nn.Dropout(dropout)  # Dropout layer\n",
        "#     #     self.fc = nn.Linear(hidden_size * 2, output_features * output_timesteps)\n",
        "#     #     self.output_features = output_features\n",
        "#     #     self.output_timesteps = output_timesteps\n",
        "\n",
        "#     # def forward(self, x):\n",
        "#     #     # x: (batch_size, seq_length, input_size)\n",
        "#     #     # _, (hn, _) = self.lstm(x)\n",
        "#     #     # # hn: (num_layers * num_directions, batch_size, hidden_size)\n",
        "#     #     # hn = hn.view(self.lstm.num_layers, 2, x.size(0), self.lstm.hidden_size)  # Reshape to access both directions\n",
        "#     #     # hn = torch.cat((hn[-1, 0], hn[-1, 1]), dim=-1)  # Concatenate forward and backward hidden states\n",
        "#     #     # hn = self.dropout(hn)  # Apply dropout\n",
        "#     #     # out = self.fc(hn)\n",
        "#     #     # return out.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, output_timesteps, output_features)\n",
        "\n",
        "#     #     _, (hn, _) = self.lstm(x)\n",
        "#     #     # hn: (num_layers, batch_size, hidden_size)\n",
        "#     #     # want to take the last layer so that you are left with (batch_size, hidden_size)\n",
        "#     #     out = self.fc(hn[-1])\n",
        "#     #     return out.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, 10, input_size)\n",
        "\n",
        "#     def __init__(self, input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1, dropout=0.2):  # Set default n to 10\n",
        "#         super(TrajectoryLSTM, self).__init__()\n",
        "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "#         self.dropout = nn.Dropout(dropout)  # Dropout layer\n",
        "#         # self.fc = nn.Linear(hidden_size, output_features * output_timesteps)\n",
        "#         self.fc = nn.Sequential(\n",
        "#           nn.Linear(hidden_size, 64),\n",
        "#           nn.BatchNorm1d(64),\n",
        "#           nn.Dropout(0.5), # Dropout after the first FC layer\n",
        "#           nn.ReLU(),\n",
        "#           nn.Linear(64, 32),\n",
        "#           nn.BatchNorm1d(32),\n",
        "#           nn.Dropout(0.3), # Dropout after the second FC layer\n",
        "#           nn.ReLU(),\n",
        "#           nn.Linear(32, output_features * output_timesteps))\n",
        "#         self.output_features = output_features\n",
        "#         self.output_timesteps = output_timesteps\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # x: (batch_size, seq_length, input_size)\n",
        "#         _, (hn, _) = self.lstm(x)\n",
        "#         # hn: (num_layers, batch_size, hidden_size)\n",
        "#         # want to take the last layer so that you are left with (batch_size, hidden_size)\n",
        "#         hn = self.dropout(hn)\n",
        "#         out = self.fc(hn[-1])\n",
        "#         return out.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, 10, input_size)\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_length, embed_dim)\n",
        "        attn_output, _ = self.attention(x, x, x)  # Self-attention mechanism\n",
        "        return attn_output\n",
        "\n",
        "class TrajectoryLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1, dropout=0.5, num_heads=1):\n",
        "        super(TrajectoryLSTM, self).__init__()\n",
        "\n",
        "        # Transform input size to match attention embed_dim if needed\n",
        "        self.input_transform = nn.Linear(input_size, 32)  # Transform input to embed_dim\n",
        "        self.dropout = nn.Dropout(dropout)  # Dropout layer\n",
        "\n",
        "        # MultiHeadSelfAttention layer\n",
        "        self.attention = MultiHeadSelfAttention(embed_dim=64, num_heads=num_heads, dropout=dropout)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "        # Fully connected layers for final prediction\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.5),  # Dropout after the first FC layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.Dropout(0.3),  # Dropout after the second FC layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, output_features * output_timesteps)\n",
        "        )\n",
        "        self.output_features = output_features\n",
        "        self.output_timesteps = output_timesteps\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Transform input to match attention's expected embed_dim\n",
        "        # transformed_x = self.input_transform(x)  # Transform input to embed_dim\n",
        "        # transformed_x: (batch_size, seq_length, embed_dim)\n",
        "\n",
        "\n",
        "        # attn_out: (batch_size, seq_length, embed_dim)\n",
        "\n",
        "        lstm_out, _ = self.lstm(x)  # Pass attention output to LSTM\n",
        "        attn_out = self.attention(lstm_out)  # Apply attention to transformed input\n",
        "        # lstm_out: (batch_size, seq_length, hidden_size)\n",
        "\n",
        "        lstm_out = self.layer_norm(lstm_out + attn_out)  # Apply LayerNorm to LSTM output\n",
        "        lstm_out = lstm_out[:, -1, :]  # Take the last time step's LSTM output\n",
        "        lstm_out = self.dropout(lstm_out)  # Apply dropout to LSTM output\n",
        "\n",
        "        out = self.fc(lstm_out)  # Fully connected layer\n",
        "        return out.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, output_timesteps, output_features)\n",
        "\n",
        "\n",
        "class TrajectoryLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TrajectoryLoss, self).__init__()\n",
        "        self.max_acceleration = 3.0\n",
        "\n",
        "    def forward(self, predictions, targets, last_input_state, dt=0.1):\n",
        "        # Reconstruct trajectory from velocities\n",
        "        pred_velocity = predictions[:, :, :]\n",
        "        cum_disp = torch.cumsum(pred_velocity * dt, dim=1)\n",
        "        # print(pred_velocity.shape)\n",
        "        # (32, 10, 2)\n",
        "\n",
        "        pred_position = cum_disp + last_input_state.unsqueeze(1)\n",
        "\n",
        "        # print(pred_position.shape)\n",
        "\n",
        "        target_position = targets[:, :, :2]\n",
        "        target_velocity = targets[:, :, 2:4]\n",
        "\n",
        "        # Calculate MSE for position\n",
        "\n",
        "        position_loss = F.mse_loss(pred_position, target_position)\n",
        "\n",
        "        velocity_loss = F.mse_loss(pred_velocity, target_velocity)\n",
        "        terminal_position_loss = F.mse_loss(pred_position[:, -1, :], target_position[:, -1, :]) + F.mse_loss(pred_position[:, 0, :], target_position[:, 0, :])\n",
        "        smoothness_loss = self.smoothness_loss(pred_position)\n",
        "\n",
        "        total_loss = position_loss + velocity_loss + smoothness_loss + 2 * terminal_position_loss\n",
        "        return total_loss\n",
        "\n",
        "    def acceleration_limit_loss(self, pred_velocity, dt=0.1):\n",
        "        # Ensure accelerations remain within feasible limits by penalizing large changes in velocity\n",
        "        approx_acceleration = (pred_velocity[:, :-1, :] - pred_velocity[:, 1:, :]) / dt  # Difference between consecutive velocities\n",
        "        acceleration_norm = torch.norm(approx_acceleration, dim=2)\n",
        "        excess_acceleration = torch.clamp(acceleration_norm - self.max_acceleration, min=0.0)\n",
        "        return torch.mean(excess_acceleration ** 2)\n",
        "\n",
        "\n",
        "    def smoothness_loss(self, pred_position):\n",
        "        # Calculate the difference between consecutive positions\n",
        "        diff = pred_position[:, :-1, :] - pred_position[:, 1:, :]\n",
        "        smoothness = torch.norm(diff, dim=2)\n",
        "        return torch.mean(smoothness)\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, initial_lr=0.001, T_max=10, freeze_attention_epochs=20):\n",
        "    criterion = TrajectoryLoss()  # Use the custom loss function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)  # Adam optimizer\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    # for param in model.attention.parameters():\n",
        "    #   param.requires_grad = False\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      # if epoch == freeze_attention_epochs:\n",
        "      #   for param in model.attention.parameters():\n",
        "          # param.requires_grad = True\n",
        "      model.train()  # Set the model to training mode\n",
        "      epoch_train_loss = 0  # Initialize training loss for the epoch\n",
        "      for batch_x, batch_y in train_loader:\n",
        "          batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "          optimizer.zero_grad()  # Zero the gradients\n",
        "          outputs = model(batch_x)  # Forward pass\n",
        "          loss = criterion(outputs, batch_y, batch_x[:, 0, :2])  # Compute loss using the custom loss function\n",
        "          loss.backward()  # Backward pass\n",
        "          optimizer.step()  # Update weights\n",
        "\n",
        "          epoch_train_loss += loss.item()  # Accumulate training loss\n",
        "\n",
        "      # Update learning rate based on the cosine decay schedule\n",
        "      scheduler.step()\n",
        "\n",
        "      # Average training loss for the epoch\n",
        "      avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "      train_losses.append(avg_train_loss)  # Store training loss\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "      # Validation loop\n",
        "      model.eval()  # Set the model to evaluation mode\n",
        "      epoch_val_loss = 0  # Initialize validation loss for the epoch\n",
        "      with torch.no_grad():\n",
        "          for batch_x, batch_y in val_loader:\n",
        "              batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "              outputs = model(batch_x)\n",
        "              loss = criterion(outputs, batch_y, batch_x[:, 0, :2])\n",
        "              epoch_val_loss += loss.item()  # Accumulate validation loss\n",
        "\n",
        "          # Average validation loss for the epoch\n",
        "          avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "          val_losses.append(avg_val_loss)  # Store validation loss\n",
        "          print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "# Create Tensor datasets\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_train.shape[2]  # Number of features\n",
        "hidden_size = 64  # You can adjust this\n",
        "# input_size = 5\n",
        "model = TrajectoryLSTM(input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1, dropout=0.5).to(\"cuda\")\n",
        "\n",
        "# Train the model with cosine decay learning rate\n",
        "train_model(model, train_loader, val_loader, num_epochs=150, initial_lr=1e-2, T_max=50)\n",
        "\n",
        "def plot_loss_curves(train_losses, val_losses):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(train_losses, label='Training Loss', color='blue')\n",
        "    plt.plot(val_losses, label='Validation Loss', color='orange')\n",
        "    plt.title('Training and Validation Loss Curves')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_curves(train_losses, val_losses)\n",
        "\n",
        "x = torch.tensor(X_val, dtype=torch.float32).to(\"cuda\")\n",
        "y = torch.tensor(y_val, dtype=torch.float32).to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(x)\n",
        "    cum_disp = torch.cumsum(outputs * 0.1, dim=1)\n",
        "    pred_position = cum_disp + y[:, 0, :2].unsqueeze(1)\n",
        "\n",
        "    distances = torch.norm(pred_position - y[:, :, :2], dim=2)\n",
        "    mean_ade = distances.mean().item()\n",
        "#LSTM\n",
        "print(\"Mean ADE:\", mean_ade)\n",
        "\n",
        "# Choose a random example from the validation dataset\n",
        "example_index = 400 # np.random.randint(0, len(X_val))\n",
        "past_traj = X_val[example_index]\n",
        "future_traj = y_val[example_index]\n",
        "\n",
        "# Get the predicted future trajectory\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  X = torch.tensor(past_traj[np.newaxis, :, :], dtype=torch.float32).to(\"cuda\")\n",
        "  predicted_vels = model(X)\n",
        "\n",
        "  predicted_vels = predicted_vels[0]\n",
        "  cum_disp = torch.cumsum(predicted_vels * 0.1, dim=1)\n",
        "  future_pred = cum_disp + X[0, -1, :2]\n",
        "  future_pred = future_pred.cpu().numpy()\n",
        "\n",
        "# print(past_traj[:, :2])\n",
        "\n",
        "\n",
        "# Plot the past trajectory, actual future trajectory, and predicted future trajectory\n",
        "#LSTM\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(past_traj[:, 0], past_traj[:, 1], label='Past Trajectory', color='blue')\n",
        "plt.plot(future_traj[:, 0], future_traj[:, 1], label='Actual Future Trajectory', color='red')\n",
        "plt.plot(future_pred[:, 0], future_pred[:, 1], label='Predicted Future Trajectory', color='green')\n",
        "plt.xlabel('X Coordinate')\n",
        "plt.ylabel('Y Coordinate')\n",
        "plt.title(f\"Predicted vs. Actual Future Trajectory (Example {example_index})\")\n",
        "plt.axis('equal')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TFuGaAOYixM1",
        "outputId": "f7770766-7235-4ed2-89e9-19d44e57b7a6"
      },
      "outputs": [],
      "source": [
        "#Kavya and Srishti\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TrajectoryLSTM(nn.Module):\n",
        "    # def __init__(self, input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1, dropout=0.2):\n",
        "    #     super(TrajectoryLSTM, self).__init__()\n",
        "    #     self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "    #     self.dropout = nn.Dropout(dropout)  # Dropout layer\n",
        "    #     self.fc = nn.Linear(hidden_size * 2, output_features * output_timesteps)\n",
        "    #     self.output_features = output_features\n",
        "    #     self.output_timesteps = output_timesteps\n",
        "\n",
        "    # def forward(self, x):\n",
        "    #     # x: (batch_size, seq_length, input_size)\n",
        "    #     # _, (hn, _) = self.lstm(x)\n",
        "    #     # # hn: (num_layers * num_directions, batch_size, hidden_size)\n",
        "    #     # hn = hn.view(self.lstm.num_layers, 2, x.size(0), self.lstm.hidden_size)  # Reshape to access both directions\n",
        "    #     # hn = torch.cat((hn[-1, 0], hn[-1, 1]), dim=-1)  # Concatenate forward and backward hidden states\n",
        "    #     # hn = self.dropout(hn)  # Apply dropout\n",
        "    #     # out = self.fc(hn)\n",
        "    #     # return out.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, output_timesteps, output_features)\n",
        "\n",
        "    #     _, (hn, _) = self.lstm(x)\n",
        "    #     # hn: (num_layers, batch_size, hidden_size)\n",
        "    #     # want to take the last layer so that you are left with (batch_size, hidden_size)\n",
        "    #     out = self.fc(hn[-1])\n",
        "    #     return out.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, 10, input_size)\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1, dropout=0.5):  # Set default n to 10\n",
        "        super(TrajectoryLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)  # Dropout layer\n",
        "        # self.fc = nn.Linear(hidden_size, output_features * output_timesteps)\n",
        "        # Single-head attention\n",
        "        self.attention = nn.Linear(hidden_size, 1)  # To compute attention scores\n",
        "        self.softmax = nn.Softmax(dim=1)  # For normalizing attention scores\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "          nn.Linear(hidden_size, 64),\n",
        "          nn.BatchNorm1d(64),\n",
        "          nn.Dropout(0.3), # Dropout after the first FC layer\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(64, 32),\n",
        "          nn.BatchNorm1d(32),\n",
        "          nn.Dropout(0.3), # Dropout after the second FC layer\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(32, output_features * output_timesteps))\n",
        "        self.output_features = output_features\n",
        "        self.output_timesteps = output_timesteps\n",
        "\n",
        "    def forward(self, x):\n",
        "        # LSTM forward pass\n",
        "        lstm_out, (hn, _) = self.lstm(x)\n",
        "        # lstm_out: (batch_size, seq_length, hidden_size)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn_scores = self.attention(lstm_out)  # (batch_size, seq_length, 1)\n",
        "        attn_weights = self.softmax(attn_scores)  # Normalize to get weights\n",
        "\n",
        "        # Apply attention weights to LSTM outputs\n",
        "        weighted_output = lstm_out * attn_weights  # (batch_size, seq_length, hidden_size)\n",
        "        context_vector = torch.sum(weighted_output, dim=1)  # Sum across seq_length\n",
        "\n",
        "        # Apply dropout and fully connected layers\n",
        "        context_vector = self.dropout(context_vector)\n",
        "        out = self.fc(context_vector)  # (batch_size, output_timesteps * output_features)\n",
        "        return out.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, output_timesteps, output_features)\n",
        "\n",
        "\n",
        "\n",
        "class TrajectoryLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TrajectoryLoss, self).__init__()\n",
        "        self.max_acceleration = 3.0\n",
        "\n",
        "    def forward(self, predictions, targets, last_input_state, dt=0.2):\n",
        "        # Reconstruct trajectory from velocities\n",
        "        pred_velocity = predictions[:, :, :]\n",
        "        cum_disp = torch.cumsum(pred_velocity * dt, dim=1)\n",
        "        # print(pred_velocity.shape)\n",
        "        # (32, 10, 2)\n",
        "\n",
        "        pred_position = cum_disp + last_input_state.unsqueeze(1)\n",
        "\n",
        "        # print(pred_position.shape)\n",
        "\n",
        "        target_position = targets[:, :, :2]\n",
        "        target_velocity = targets[:, :, 2:4]\n",
        "\n",
        "        # Calculate MSE for position\n",
        "\n",
        "        position_loss = F.mse_loss(pred_position, target_position)\n",
        "\n",
        "        velocity_loss = F.mse_loss(pred_velocity, target_velocity)\n",
        "        terminal_position_loss = F.mse_loss(pred_position[:, -1, :], target_position[:, -1, :]) + F.mse_loss(pred_position[:, 0, :], target_position[:, 0, :])\n",
        "        smoothness_loss = self.smoothness_loss(pred_position)\n",
        "\n",
        "        # total_loss = position_loss + velocity_loss + smoothness_loss + 2 * terminal_position_loss\n",
        "        total_loss = 2 * position_loss + velocity_loss + 0.5 * smoothness_loss + 3 * terminal_position_loss\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def acceleration_limit_loss(self, pred_velocity, dt=0.1):\n",
        "        # Ensure accelerations remain within feasible limits by penalizing large changes in velocity\n",
        "        approx_acceleration = (pred_velocity[:, :-1, :] - pred_velocity[:, 1:, :]) / dt  # Difference between consecutive velocities\n",
        "        acceleration_norm = torch.norm(approx_acceleration, dim=2)\n",
        "        excess_acceleration = torch.clamp(acceleration_norm - self.max_acceleration, min=0.0)\n",
        "        return torch.mean(excess_acceleration ** 2)\n",
        "\n",
        "\n",
        "    def smoothness_loss(self, pred_position):\n",
        "        # Calculate the difference between consecutive positions\n",
        "        diff = pred_position[:, :-1, :] - pred_position[:, 1:, :]\n",
        "        smoothness = torch.norm(diff, dim=2)\n",
        "        return torch.mean(smoothness)\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, initial_lr=0.001, T_max=10):\n",
        "    criterion = TrajectoryLoss()  # Use the custom loss function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)  # Adam optimizer\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        epoch_train_loss = 0  # Initialize training loss for the epoch\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            outputs = model(batch_x)  # Forward pass\n",
        "            loss = criterion(outputs, batch_y, batch_x[:, 0, :2])  # Compute loss using the custom loss function\n",
        "            loss.backward()  # Backward pass\n",
        "            # added this to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=3)\n",
        "\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            epoch_train_loss += loss.item()  # Accumulate training loss\n",
        "\n",
        "        # Update learning rate based on the cosine decay schedule\n",
        "        scheduler.step()\n",
        "\n",
        "        # Average training loss for the epoch\n",
        "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)  # Store training loss\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        epoch_val_loss = 0  # Initialize validation loss for the epoch\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "                outputs = model(batch_x)\n",
        "                loss = criterion(outputs, batch_y, batch_x[:, 0, :2])\n",
        "                epoch_val_loss += loss.item()  # Accumulate validation loss\n",
        "\n",
        "            # Average validation loss for the epoch\n",
        "            avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "            val_losses.append(avg_val_loss)  # Store validation loss\n",
        "            print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "# Create Tensor datasets\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_train.shape[2]  # Number of features\n",
        "hidden_size = 128  # You can adjust this\n",
        "# input_size = 5\n",
        "model = TrajectoryLSTM(input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=2, dropout=0.5).to(\"cuda\")\n",
        "\n",
        "# Train the model with cosine decay learning rate\n",
        "train_model(model, train_loader, val_loader, num_epochs=150, initial_lr=1e-2, T_max=50)\n",
        "\n",
        "def plot_loss_curves(train_losses, val_losses):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(train_losses, label='Training Loss', color='blue')\n",
        "    plt.plot(val_losses, label='Validation Loss', color='orange')\n",
        "    plt.title('Training and Validation Loss Curves')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_curves(train_losses, val_losses)\n",
        "\n",
        "x = torch.tensor(X_val, dtype=torch.float32).to(\"cuda\")\n",
        "y = torch.tensor(y_val, dtype=torch.float32).to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(x)\n",
        "    cum_disp = torch.cumsum(outputs * 0.1, dim=1)\n",
        "    pred_position = cum_disp + y[:, 0, :2].unsqueeze(1)\n",
        "\n",
        "    distances = torch.norm(pred_position - y[:, :, :2], dim=2)\n",
        "    mean_ade = distances.mean().item()\n",
        "#LSTM\n",
        "print(\"Mean ADE:\", mean_ade)\n",
        "\n",
        "# Choose a random example from the validation dataset\n",
        "example_index = 400 # np.random.randint(0, len(X_val))\n",
        "past_traj = X_val[example_index]\n",
        "future_traj = y_val[example_index]\n",
        "\n",
        "# Get the predicted future trajectory\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  X = torch.tensor(past_traj[np.newaxis, :, :], dtype=torch.float32).to(\"cuda\")\n",
        "  predicted_vels = model(X)\n",
        "\n",
        "  predicted_vels = predicted_vels[0]\n",
        "  cum_disp = torch.cumsum(predicted_vels * 0.1, dim=1)\n",
        "  future_pred = cum_disp + X[0, -1, :2]\n",
        "  future_pred = future_pred.cpu().numpy()\n",
        "\n",
        "# print(past_traj[:, :2])\n",
        "\n",
        "\n",
        "# Plot the past trajectory, actual future trajectory, and predicted future trajectory\n",
        "#LSTM\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(past_traj[:, 0], past_traj[:, 1], label='Past Trajectory', color='blue')\n",
        "plt.plot(future_traj[:, 0], future_traj[:, 1], label='Actual Future Trajectory', color='red')\n",
        "plt.plot(future_pred[:, 0], future_pred[:, 1], label='Predicted Future Trajectory', color='green')\n",
        "plt.xlabel('X Coordinate')\n",
        "plt.ylabel('Y Coordinate')\n",
        "plt.title(f\"Predicted vs. Actual Future Trajectory (Example {example_index})\")\n",
        "plt.axis('equal')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBf1UXVpU7Oq"
      },
      "source": [
        "# Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gHw6ON2xSwCx",
        "outputId": "b5a4d0e3-62a2-405e-e66b-d725796094cd"
      },
      "outputs": [],
      "source": [
        "#Kavya and Srishti\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TrajectoryLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1, dropout=0.5):\n",
        "        super(TrajectoryLSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_features = output_features\n",
        "        self.output_timesteps = output_timesteps\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)  # Dropout layer\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Linear(hidden_size, 1)  # To compute attention scores\n",
        "        self.softmax = nn.Softmax(dim=1)  # For normalizing attention scores\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, output_features * output_timesteps)\n",
        "        )\n",
        "\n",
        "    def positional_encoding(self, seq_length, dim):\n",
        "        # Create the positional encoding matrix\n",
        "        pos = torch.arange(seq_length, dtype=torch.float32).unsqueeze(1)\n",
        "        i = torch.arange(dim, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        # Compute sinusoidal functions\n",
        "        angle_rates = 1 / (10000 ** (2 * (i // 2) / dim))\n",
        "        angle_rads = pos * angle_rates\n",
        "\n",
        "        # Apply sin to even indices and cos to odd indices\n",
        "        pos_enc = torch.zeros((seq_length, dim))\n",
        "        pos_enc[:, 0::2] = torch.sin(angle_rads[:, 0::2])  # sin for even indices\n",
        "        pos_enc[:, 1::2] = torch.cos(angle_rads[:, 1::2])  # cos for odd indices\n",
        "\n",
        "        return pos_enc\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "\n",
        "        # Add positional encoding\n",
        "        pos_enc = self.positional_encoding(seq_length, self.input_size).to(x.device)\n",
        "        x = x + pos_enc.unsqueeze(0)  # Broadcast to batch size\n",
        "\n",
        "        # LSTM forward pass\n",
        "        lstm_out, (hn, _) = self.lstm(x)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn_scores = self.attention(lstm_out)  # (batch_size, seq_length, 1)\n",
        "        attn_weights = self.softmax(attn_scores)  # Normalize to get weights\n",
        "\n",
        "        # Apply attention weights to LSTM outputs\n",
        "        weighted_output = lstm_out * attn_weights  # (batch_size, seq_length, hidden_size)\n",
        "        context_vector = torch.sum(weighted_output, dim=1)  # Sum across seq_length\n",
        "\n",
        "        # Apply dropout and fully connected layers\n",
        "        context_vector = self.dropout(context_vector)\n",
        "        out = self.fc(context_vector)  # (batch_size, output_timesteps * output_features)\n",
        "        return out.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, output_timesteps, output_features)\n",
        "\n",
        "\n",
        "class TrajectoryLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TrajectoryLoss, self).__init__()\n",
        "        self.max_acceleration = 3.0\n",
        "\n",
        "    def forward(self, predictions, targets, last_input_state, dt=0.2):\n",
        "        # Reconstruct trajectory from velocities\n",
        "        pred_velocity = predictions[:, :, :]\n",
        "        cum_disp = torch.cumsum(pred_velocity * dt, dim=1)\n",
        "        # print(pred_velocity.shape)\n",
        "        # (32, 10, 2)\n",
        "\n",
        "        pred_position = cum_disp + last_input_state.unsqueeze(1)\n",
        "\n",
        "        # print(pred_position.shape)\n",
        "\n",
        "        target_position = targets[:, :, :2]\n",
        "        target_velocity = targets[:, :, 2:4]\n",
        "\n",
        "        # Calculate MSE for position\n",
        "\n",
        "        position_loss = F.mse_loss(pred_position, target_position)\n",
        "\n",
        "        velocity_loss = F.mse_loss(pred_velocity, target_velocity)\n",
        "        terminal_position_loss = F.mse_loss(pred_position[:, -1, :], target_position[:, -1, :]) + F.mse_loss(pred_position[:, 0, :], target_position[:, 0, :])\n",
        "        smoothness_loss = self.smoothness_loss(pred_position)\n",
        "\n",
        "        # total_loss = position_loss + velocity_loss + smoothness_loss + 2 * terminal_position_loss\n",
        "        total_loss = 2 * position_loss + velocity_loss + 0.5 * smoothness_loss + 2 * terminal_position_loss\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def acceleration_limit_loss(self, pred_velocity, dt=0.1):\n",
        "        # Ensure accelerations remain within feasible limits by penalizing large changes in velocity\n",
        "        approx_acceleration = (pred_velocity[:, :-1, :] - pred_velocity[:, 1:, :]) / dt  # Difference between consecutive velocities\n",
        "        acceleration_norm = torch.norm(approx_acceleration, dim=2)\n",
        "        excess_acceleration = torch.clamp(acceleration_norm - self.max_acceleration, min=0.0)\n",
        "        return torch.mean(excess_acceleration ** 2)\n",
        "\n",
        "\n",
        "    def smoothness_loss(self, pred_position):\n",
        "        # Calculate the difference between consecutive positions\n",
        "        diff = pred_position[:, :-1, :] - pred_position[:, 1:, :]\n",
        "        smoothness = torch.norm(diff, dim=2)\n",
        "        return torch.mean(smoothness)\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, initial_lr=0.001, T_max=10):\n",
        "    criterion = TrajectoryLoss()  # Use the custom loss function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)  # Adam optimizer\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        epoch_train_loss = 0  # Initialize training loss for the epoch\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            outputs = model(batch_x)  # Forward pass\n",
        "            loss = criterion(outputs, batch_y, batch_x[:, 0, :2])  # Compute loss using the custom loss function\n",
        "            loss.backward()  # Backward pass\n",
        "            # added this to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=3)\n",
        "\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            epoch_train_loss += loss.item()  # Accumulate training loss\n",
        "\n",
        "        # Update learning rate based on the cosine decay schedule\n",
        "        scheduler.step()\n",
        "\n",
        "        # Average training loss for the epoch\n",
        "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)  # Store training loss\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        epoch_val_loss = 0  # Initialize validation loss for the epoch\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "                outputs = model(batch_x)\n",
        "                loss = criterion(outputs, batch_y, batch_x[:, 0, :2])\n",
        "                epoch_val_loss += loss.item()  # Accumulate validation loss\n",
        "\n",
        "            # Average validation loss for the epoch\n",
        "            avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "            val_losses.append(avg_val_loss)  # Store validation loss\n",
        "            print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "# Create Tensor datasets\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_train.shape[2]  # Number of features\n",
        "hidden_size = 128  # You can adjust this\n",
        "# input_size = 5\n",
        "model = TrajectoryLSTM(input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1, dropout=0.5).to(\"cuda\")\n",
        "\n",
        "# Train the model with cosine decay learning rate\n",
        "train_model(model, train_loader, val_loader, num_epochs=150, initial_lr=1e-2, T_max=50)\n",
        "\n",
        "def plot_loss_curves(train_losses, val_losses):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(train_losses, label='Training Loss', color='blue')\n",
        "    plt.plot(val_losses, label='Validation Loss', color='orange')\n",
        "    plt.title('Training and Validation Loss Curves')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_curves(train_losses, val_losses)\n",
        "\n",
        "x = torch.tensor(X_val, dtype=torch.float32).to(\"cuda\")\n",
        "y = torch.tensor(y_val, dtype=torch.float32).to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(x)\n",
        "    cum_disp = torch.cumsum(outputs * 0.1, dim=1)\n",
        "    pred_position = cum_disp + y[:, 0, :2].unsqueeze(1)\n",
        "\n",
        "    distances = torch.norm(pred_position - y[:, :, :2], dim=2)\n",
        "    mean_ade = distances.mean().item()\n",
        "#LSTM\n",
        "print(\"Mean ADE:\", mean_ade)\n",
        "\n",
        "# Choose a random example from the validation dataset\n",
        "example_index = 400 # np.random.randint(0, len(X_val))\n",
        "past_traj = X_val[example_index]\n",
        "future_traj = y_val[example_index]\n",
        "\n",
        "# Get the predicted future trajectory\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  X = torch.tensor(past_traj[np.newaxis, :, :], dtype=torch.float32).to(\"cuda\")\n",
        "  predicted_vels = model(X)\n",
        "\n",
        "  predicted_vels = predicted_vels[0]\n",
        "  cum_disp = torch.cumsum(predicted_vels * 0.1, dim=1)\n",
        "  future_pred = cum_disp + X[0, -1, :2]\n",
        "  future_pred = future_pred.cpu().numpy()\n",
        "\n",
        "# print(past_traj[:, :2])\n",
        "\n",
        "\n",
        "# Plot the past trajectory, actual future trajectory, and predicted future trajectory\n",
        "#LSTM\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(past_traj[:, 0], past_traj[:, 1], label='Past Trajectory', color='blue')\n",
        "plt.plot(future_traj[:, 0], future_traj[:, 1], label='Actual Future Trajectory', color='red')\n",
        "plt.plot(future_pred[:, 0], future_pred[:, 1], label='Predicted Future Trajectory', color='green')\n",
        "plt.xlabel('X Coordinate')\n",
        "plt.ylabel('Y Coordinate')\n",
        "plt.title(f\"Predicted vs. Actual Future Trajectory (Example {example_index})\")\n",
        "plt.axis('equal')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNAIELY790YQ"
      },
      "source": [
        "# External Actors Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4H4G80qj7Kim",
        "outputId": "a2e489a2-6705-41e6-d0e4-762b1e0a398f"
      },
      "outputs": [],
      "source": [
        "#Kavya and Srishti\n",
        "# External Actors added\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TrajectoryLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1, dropout=0.5):\n",
        "        super(TrajectoryLSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_features = output_features\n",
        "        self.output_timesteps = output_timesteps\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)  # Dropout layer\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Linear(hidden_size, 1)  # To compute attention scores\n",
        "        self.softmax = nn.Softmax(dim=1)  # For normalizing attention scores\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, output_features * output_timesteps)\n",
        "        )\n",
        "\n",
        "    def positional_encoding(self, seq_length, dim):\n",
        "        # Create the positional encoding matrix\n",
        "        pos = torch.arange(seq_length, dtype=torch.float32).unsqueeze(1)\n",
        "        i = torch.arange(dim, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        # Compute sinusoidal functions\n",
        "        angle_rates = 1 / (10000 ** (2 * (i // 2) / dim))\n",
        "        angle_rads = pos * angle_rates\n",
        "\n",
        "        # Apply sin to even indices and cos to odd indices\n",
        "        pos_enc = torch.zeros((seq_length, dim))\n",
        "        pos_enc[:, 0::2] = torch.sin(angle_rads[:, 0::2])  # sin for even indices\n",
        "        pos_enc[:, 1::2] = torch.cos(angle_rads[:, 1::2])  # cos for odd indices\n",
        "\n",
        "        return pos_enc\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "\n",
        "        # Add positional encoding\n",
        "        pos_enc = self.positional_encoding(seq_length, self.input_size).to(x.device)\n",
        "        x = x + pos_enc.unsqueeze(0)  # Broadcast to batch size\n",
        "\n",
        "        # LSTM forward pass\n",
        "        lstm_out, (hn, _) = self.lstm(x)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn_scores = self.attention(lstm_out)  # (batch_size, seq_length, 1)\n",
        "        attn_weights = self.softmax(attn_scores)  # Normalize to get weights\n",
        "\n",
        "        # Apply attention weights to LSTM outputs\n",
        "        weighted_output = lstm_out * attn_weights  # (batch_size, seq_length, hidden_size)\n",
        "        context_vector = torch.sum(weighted_output, dim=1)  # Sum across seq_length\n",
        "\n",
        "        # Apply dropout and fully connected layers\n",
        "        context_vector = self.dropout(context_vector)\n",
        "        out = self.fc(context_vector)  # (batch_size, output_timesteps * output_features)\n",
        "        return out.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, output_timesteps, output_features)\n",
        "\n",
        "\n",
        "class TrajectoryLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TrajectoryLoss, self).__init__()\n",
        "        self.max_acceleration = 3.0\n",
        "\n",
        "    def forward(self, predictions, targets, last_input_state, dt=0.2):\n",
        "        # Reconstruct trajectory from velocities\n",
        "        pred_velocity = predictions[:, :, :]\n",
        "        cum_disp = torch.cumsum(pred_velocity * dt, dim=1)\n",
        "        # print(pred_velocity.shape)\n",
        "        # (32, 10, 2)\n",
        "\n",
        "        pred_position = cum_disp + last_input_state.unsqueeze(1)\n",
        "\n",
        "        # print(pred_position.shape)\n",
        "\n",
        "        target_position = targets[:, :, :2]\n",
        "        target_velocity = targets[:, :, 2:4]\n",
        "\n",
        "        # Calculate MSE for position\n",
        "\n",
        "        position_loss = F.mse_loss(pred_position, target_position)\n",
        "\n",
        "        velocity_loss = F.mse_loss(pred_velocity, target_velocity)\n",
        "        terminal_position_loss = F.mse_loss(pred_position[:, -1, :], target_position[:, -1, :]) + F.mse_loss(pred_position[:, 0, :], target_position[:, 0, :])\n",
        "        smoothness_loss = self.smoothness_loss(pred_position)\n",
        "\n",
        "        # total_loss = position_loss + velocity_loss + smoothness_loss + 2 * terminal_position_loss\n",
        "        total_loss = 2 * position_loss + velocity_loss + 0.5 * smoothness_loss + 2 * terminal_position_loss\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def acceleration_limit_loss(self, pred_velocity, dt=0.1):\n",
        "        # Ensure accelerations remain within feasible limits by penalizing large changes in velocity\n",
        "        approx_acceleration = (pred_velocity[:, :-1, :] - pred_velocity[:, 1:, :]) / dt  # Difference between consecutive velocities\n",
        "        acceleration_norm = torch.norm(approx_acceleration, dim=2)\n",
        "        excess_acceleration = torch.clamp(acceleration_norm - self.max_acceleration, min=0.0)\n",
        "        return torch.mean(excess_acceleration ** 2)\n",
        "\n",
        "\n",
        "    def smoothness_loss(self, pred_position):\n",
        "        # Calculate the difference between consecutive positions\n",
        "        diff = pred_position[:, :-1, :] - pred_position[:, 1:, :]\n",
        "        smoothness = torch.norm(diff, dim=2)\n",
        "        return torch.mean(smoothness)\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, initial_lr=0.001, T_max=10):\n",
        "    criterion = TrajectoryLoss()  # Use the custom loss function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)  # Adam optimizer\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        epoch_train_loss = 0  # Initialize training loss for the epoch\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            outputs = model(batch_x)  # Forward pass\n",
        "            loss = criterion(outputs, batch_y, batch_x[:, 0, :2])  # Compute loss using the custom loss function\n",
        "            loss.backward()  # Backward pass\n",
        "            # added this to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=3)\n",
        "\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            epoch_train_loss += loss.item()  # Accumulate training loss\n",
        "\n",
        "        # Update learning rate based on the cosine decay schedule\n",
        "        scheduler.step()\n",
        "\n",
        "        # Average training loss for the epoch\n",
        "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)  # Store training loss\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        epoch_val_loss = 0  # Initialize validation loss for the epoch\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "                outputs = model(batch_x)\n",
        "                loss = criterion(outputs, batch_y, batch_x[:, 0, :2])\n",
        "                epoch_val_loss += loss.item()  # Accumulate validation loss\n",
        "\n",
        "            # Average validation loss for the epoch\n",
        "            avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "            val_losses.append(avg_val_loss)  # Store validation loss\n",
        "            print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "# Create Tensor datasets\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_train.shape[2]  # Number of features\n",
        "hidden_size = 128  # You can adjust this\n",
        "# input_size = 5\n",
        "model = TrajectoryLSTM(input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1, dropout=0.5).to(\"cuda\")\n",
        "\n",
        "# Train the model with cosine decay learning rate\n",
        "train_model(model, train_loader, val_loader, num_epochs=150, initial_lr=1e-2, T_max=50)\n",
        "\n",
        "def plot_loss_curves(train_losses, val_losses):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(train_losses, label='Training Loss', color='blue')\n",
        "    plt.plot(val_losses, label='Validation Loss', color='orange')\n",
        "    plt.title('Training and Validation Loss Curves')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_curves(train_losses, val_losses)\n",
        "\n",
        "x = torch.tensor(X_val, dtype=torch.float32).to(\"cuda\")\n",
        "y = torch.tensor(y_val, dtype=torch.float32).to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(x)\n",
        "    cum_disp = torch.cumsum(outputs * 0.1, dim=1)\n",
        "    pred_position = cum_disp + y[:, 0, :2].unsqueeze(1)\n",
        "\n",
        "    distances = torch.norm(pred_position - y[:, :, :2], dim=2)\n",
        "    mean_ade = distances.mean().item()\n",
        "#LSTM\n",
        "print(\"Mean ADE:\", mean_ade)\n",
        "\n",
        "# Choose a random example from the validation dataset\n",
        "example_index = 400 # np.random.randint(0, len(X_val))\n",
        "past_traj = X_val[example_index]\n",
        "future_traj = y_val[example_index]\n",
        "\n",
        "# Get the predicted future trajectory\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  X = torch.tensor(past_traj[np.newaxis, :, :], dtype=torch.float32).to(\"cuda\")\n",
        "  predicted_vels = model(X)\n",
        "\n",
        "  predicted_vels = predicted_vels[0]\n",
        "  cum_disp = torch.cumsum(predicted_vels * 0.1, dim=1)\n",
        "  future_pred = cum_disp + X[0, -1, :2]\n",
        "  future_pred = future_pred.cpu().numpy()\n",
        "\n",
        "# print(past_traj[:, :2])\n",
        "\n",
        "\n",
        "# Plot the past trajectory, actual future trajectory, and predicted future trajectory\n",
        "#LSTM\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(past_traj[:, 0], past_traj[:, 1], label='Past Trajectory', color='blue')\n",
        "plt.plot(future_traj[:, 0], future_traj[:, 1], label='Actual Future Trajectory', color='red')\n",
        "plt.plot(future_pred[:, 0], future_pred[:, 1], label='Predicted Future Trajectory', color='green')\n",
        "plt.xlabel('X Coordinate')\n",
        "plt.ylabel('Y Coordinate')\n",
        "plt.title(f\"Predicted vs. Actual Future Trajectory (Example {example_index})\")\n",
        "plt.axis('equal')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
