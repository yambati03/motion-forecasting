{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "acHO-qUaeftG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91bBLyrLel-3",
        "outputId": "78ed9f98-9292-482d-9ff9-733ef617b9c2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D06I7HvLeupM"
      },
      "source": [
        "This code below is used to index into the data and retrieve the 14 features. The last timestep is ignored. Look at the comments below to see how to retrieve the subset of features - the original vehicle features will correspond to 1, 2, 3, 4, 5, while the pedestrian features correspond to 6 and 7, the cyclist is 8 and 9, and the roadgraph features are 10, 11, 12, and 13. We played around including a subset of the features since utilizing all 13 features produced bad results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoBPQELPepyt",
        "outputId": "a2ee1bc2-09b7-4fb7-8287-e8c0e10e5c14"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "data_path = 'parsed_data_final_srishti.pkl'\n",
        "with open(data_path, \"rb\") as file:\n",
        "  data = pickle.load(file)\n",
        "\n",
        "print(data.shape)\n",
        "\n",
        "data = data[:, :-1, :]\n",
        "\n",
        "# This is the for all the features\n",
        "# data = data[:, :, 1:]\n",
        "\n",
        "#This is for vehicle features only\n",
        "data = data[:, :, 1:6]\n",
        "\n",
        "# This is for vehicle + pedestrians only\n",
        "# data = data[:, :, 1:8]\n",
        "\n",
        "\n",
        "# This is for vehicle + pedestrians + roadgraphs\n",
        "# another_data = data[:, :, 1:8]\n",
        "# new_data = data[:, :, 10:]\n",
        "# data = np.concatenate((another_data, new_data), axis = 2)\n",
        "\n",
        "\n",
        "print(f\"Shape of full dataset: {data.shape}\")\n",
        "# print(data[0])\n",
        "X_train = data[:int(0.8*len(data)), :80, :]\n",
        "X_val = data[int(0.8*len(data)): , :80, :]\n",
        "\n",
        "y_train = data[:int(0.8*len(data)), 80:, :]\n",
        "y_val = data[int(0.8*len(data)): , 80:, :]\n",
        "\n",
        "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--cmJFTCe8Nt"
      },
      "source": [
        "# Initial External Actors Implementation - Base Model\n",
        "\n",
        "\n",
        "Vehicle features + pedestrian features + cyclist features + roadgraph features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v6bmkt6Ve_6D",
        "outputId": "14fba1ef-7bab-40c0-a23a-2c40cfa59954"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "data_path = 'parsed_data_final_srishti.pkl'\n",
        "with open(data_path, \"rb\") as file:\n",
        "  data = pickle.load(file)\n",
        "\n",
        "print(data.shape)\n",
        "\n",
        "data = data[:, :-1, :]\n",
        "\n",
        "# This is the for all the features\n",
        "# data = data[:, :, 1:]\n",
        "\n",
        "#This is for vehicle features only\n",
        "data = data[:, :, 1:6]\n",
        "\n",
        "# This is for vehicle + pedestrians only\n",
        "# data = data[:, :, 1:8]\n",
        "\n",
        "\n",
        "# This is for vehicle + pedestrians + roadgraphs\n",
        "# another_data = data[:, :, 1:8]\n",
        "# new_data = data[:, :, 10:]\n",
        "# data = np.concatenate((another_data, new_data), axis = 2)\n",
        "\n",
        "\n",
        "print(f\"Shape of full dataset: {data.shape}\")\n",
        "# print(data[0])\n",
        "X_train = data[:int(0.8*len(data)), :80, :]\n",
        "X_val = data[int(0.8*len(data)): , :80, :]\n",
        "\n",
        "y_train = data[:int(0.8*len(data)), 80:, :]\n",
        "y_val = data[int(0.8*len(data)): , 80:, :]\n",
        "\n",
        "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TrajectoryLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1):  # Set default n to 10\n",
        "        super(TrajectoryLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_features * output_timesteps)\n",
        "        self.output_features = output_features\n",
        "        self.output_timesteps = output_timesteps\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_length, input_size)\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        # hn: (num_layers, batch_size, hidden_size)\n",
        "        # want to take the last layer so that you are left with (batch_size, hidden_size)\n",
        "        out = self.fc(hn[-1])\n",
        "        return out.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, 10, input_size)\n",
        "\n",
        "\n",
        "class TrajectoryLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TrajectoryLoss, self).__init__()\n",
        "        self.max_acceleration = 3.0\n",
        "\n",
        "    def forward(self, predictions, targets, last_input_state, dt=0.1):\n",
        "        # Reconstruct trajectory from velocities\n",
        "        pred_velocity = predictions[:, :, :]\n",
        "        cum_disp = torch.cumsum(pred_velocity * dt, dim=1)\n",
        "        # print(pred_velocity.shape)\n",
        "        # (32, 10, 2)\n",
        "        pred_position = cum_disp + last_input_state.unsqueeze(1)\n",
        "        # print(pred_position.shape)\n",
        "\n",
        "        target_position = targets[:, :, :2]\n",
        "        target_velocity = targets[:, :, 2:4]\n",
        "\n",
        "        # Calculate MSE for position\n",
        "\n",
        "        position_loss = F.mse_loss(pred_position, target_position)\n",
        "        velocity_loss = F.mse_loss(pred_velocity, target_velocity)\n",
        "        terminal_position_loss = F.mse_loss(pred_position[:, -1, :], target_position[:, -1, :]) + F.mse_loss(pred_position[:, 0, :], target_position[:, 0, :])\n",
        "        smoothness_loss = self.smoothness_loss(pred_position)\n",
        "\n",
        "        total_loss = position_loss + velocity_loss + smoothness_loss + 2 * terminal_position_loss\n",
        "        return total_loss\n",
        "\n",
        "    def acceleration_limit_loss(self, pred_velocity, dt=0.1):\n",
        "        # Ensure accelerations remain within feasible limits by penalizing large changes in velocity\n",
        "        approx_acceleration = (pred_velocity[:, :-1, :] - pred_velocity[:, 1:, :]) / dt  # Difference between consecutive velocities\n",
        "        acceleration_norm = torch.norm(approx_acceleration, dim=2)\n",
        "        excess_acceleration = torch.clamp(acceleration_norm - self.max_acceleration, min=0.0)\n",
        "        return torch.mean(excess_acceleration ** 2)\n",
        "\n",
        "\n",
        "    def smoothness_loss(self, pred_position):\n",
        "        # Calculate the difference between consecutive positions\n",
        "        diff = pred_position[:, :-1, :] - pred_position[:, 1:, :]\n",
        "        smoothness = torch.norm(diff, dim=2)\n",
        "        return torch.mean(smoothness)\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, initial_lr=0.001, T_max=10):\n",
        "    criterion = TrajectoryLoss()  # Use the custom loss function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)  # Adam optimizer\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        epoch_train_loss = 0  # Initialize training loss for the epoch\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            outputs = model(batch_x)  # Forward pass\n",
        "            loss = criterion(outputs, batch_y, batch_x[:, 0, :2])  # Compute loss using the custom loss function\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            epoch_train_loss += loss.item()  # Accumulate training loss\n",
        "\n",
        "        # Update learning rate based on the cosine decay schedule\n",
        "        scheduler.step()\n",
        "\n",
        "        # Average training loss for the epoch\n",
        "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)  # Store training loss\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        epoch_val_loss = 0  # Initialize validation loss for the epoch\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "                outputs = model(batch_x)\n",
        "                loss = criterion(outputs, batch_y, batch_x[:, 0, :2])\n",
        "                epoch_val_loss += loss.item()  # Accumulate validation loss\n",
        "\n",
        "            # Average validation loss for the epoch\n",
        "            avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "            val_losses.append(avg_val_loss)  # Store validation loss\n",
        "            print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "# Create Tensor datasets\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_train.shape[2]  # Number of features\n",
        "# hidden_size = 64  # You can adjust this\n",
        "\n",
        "hidden_size = 32  # You can adjust this\n",
        "\n",
        "# input_size = 5\n",
        "# model = TrajectoryLSTM(input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=4).to(\"cuda\")\n",
        "model = TrajectoryLSTM(input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1).to(\"cuda\")\n",
        "\n",
        "\n",
        "# Train the model with cosine decay learning rate\n",
        "\n",
        "\n",
        "train_model(model, train_loader, val_loader, num_epochs=150, initial_lr=1e-2, T_max=50)\n",
        "\n",
        "\n",
        "x = torch.tensor(X_val, dtype=torch.float32).to(\"cuda\")\n",
        "y = torch.tensor(y_val, dtype=torch.float32).to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(x)\n",
        "    cum_disp = torch.cumsum(outputs * 0.1, dim=1)\n",
        "    pred_position = cum_disp + y[:, 0, :2].unsqueeze(1)\n",
        "\n",
        "    distances = torch.norm(pred_position - y[:, :, :2], dim=2)\n",
        "    mean_ade = distances.mean().item()\n",
        "#LSTM\n",
        "print(\"Mean ADE:\", mean_ade)\n",
        "\n",
        "# Choose a random example from the validation dataset\n",
        "example_index = 400 # np.random.randint(0, len(X_val))\n",
        "# 300 produces no movement for x and y\n",
        "# 250 produces an erratic motion for x and y (curvy)\n",
        "\n",
        "past_traj = X_val[example_index]\n",
        "future_traj = y_val[example_index]\n",
        "\n",
        "# Get the predicted future trajectory\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  X = torch.tensor(past_traj[np.newaxis, :, :], dtype=torch.float32).to(\"cuda\")\n",
        "  predicted_vels = model(X)\n",
        "\n",
        "  predicted_vels = predicted_vels[0]\n",
        "  cum_disp = torch.cumsum(predicted_vels * 0.1, dim=1)\n",
        "  future_pred = cum_disp + X[0, -1, :2]\n",
        "  future_pred = future_pred.cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "# Plot the past trajectory, actual future trajectory, and predicted future trajectory\n",
        "#LSTM\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(past_traj[:, 0], past_traj[:, 1], label='Past Trajectory', color='blue')\n",
        "plt.plot(future_traj[:, 0], future_traj[:, 1], label='Actual Future Trajectory', color='red')\n",
        "plt.plot(future_pred[:, 0], future_pred[:, 1], label='Predicted Future Trajectory', color='green')\n",
        "plt.xlabel('X Coordinate')\n",
        "plt.ylabel('Y Coordinate')\n",
        "plt.title(f\"Predicted vs. Actual Future Trajectory (Example {example_index})\")\n",
        "plt.axis('equal')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# print(past_traj[:, :2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbYf0FaQfaPX"
      },
      "source": [
        "\n",
        "## Vehicle features + pedestrian features + roadgraph features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mNHvw6jUiAOi",
        "outputId": "a961f654-215e-4334-bb97-c9b2674f9d2e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "data_path = '/content/drive/MyDrive/CS7643Group/Dataset/training/parsed_data_final_srishti.pkl'\n",
        "with open(data_path, \"rb\") as file:\n",
        "  data = pickle.load(file)\n",
        "\n",
        "print(data.shape)\n",
        "\n",
        "data = data[:, :-1, :]\n",
        "\n",
        "# This is the for all the features\n",
        "# data = data[:, :, 1:]\n",
        "\n",
        "#This is for vehicle features only\n",
        "data = data[:, :, 1:6]\n",
        "\n",
        "# This is for vehicle + pedestrians only\n",
        "# data = data[:, :, 1:8]\n",
        "\n",
        "\n",
        "# This is for vehicle + pedestrians + roadgraphs\n",
        "# another_data = data[:, :, 1:8]\n",
        "# new_data = data[:, :, 10:]\n",
        "# data = np.concatenate((another_data, new_data), axis = 2)\n",
        "\n",
        "\n",
        "print(f\"Shape of full dataset: {data.shape}\")\n",
        "# print(data[0])\n",
        "X_train = data[:int(0.8*len(data)), :80, :]\n",
        "X_val = data[int(0.8*len(data)): , :80, :]\n",
        "\n",
        "y_train = data[:int(0.8*len(data)), 80:, :]\n",
        "y_val = data[int(0.8*len(data)): , 80:, :]\n",
        "\n",
        "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TrajectoryLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1):  # Set default n to 10\n",
        "        super(TrajectoryLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_features * output_timesteps)\n",
        "        self.output_features = output_features\n",
        "        self.output_timesteps = output_timesteps\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_length, input_size)\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        # hn: (num_layers, batch_size, hidden_size)\n",
        "        # want to take the last layer so that you are left with (batch_size, hidden_size)\n",
        "        out = self.fc(hn[-1])\n",
        "        return out.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, 10, input_size)\n",
        "\n",
        "\n",
        "class TrajectoryLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TrajectoryLoss, self).__init__()\n",
        "        self.max_acceleration = 3.0\n",
        "\n",
        "    def forward(self, predictions, targets, last_input_state, dt=0.1):\n",
        "        # Reconstruct trajectory from velocities\n",
        "        pred_velocity = predictions[:, :, :]\n",
        "        cum_disp = torch.cumsum(pred_velocity * dt, dim=1)\n",
        "        # print(pred_velocity.shape)\n",
        "        # (32, 10, 2)\n",
        "        pred_position = cum_disp + last_input_state.unsqueeze(1)\n",
        "        # print(pred_position.shape)\n",
        "\n",
        "        target_position = targets[:, :, :2]\n",
        "        target_velocity = targets[:, :, 2:4]\n",
        "\n",
        "        # Calculate MSE for position\n",
        "\n",
        "        position_loss = F.mse_loss(pred_position, target_position)\n",
        "        velocity_loss = F.mse_loss(pred_velocity, target_velocity)\n",
        "        terminal_position_loss = F.mse_loss(pred_position[:, -1, :], target_position[:, -1, :]) + F.mse_loss(pred_position[:, 0, :], target_position[:, 0, :])\n",
        "        smoothness_loss = self.smoothness_loss(pred_position)\n",
        "\n",
        "        total_loss = position_loss + velocity_loss + smoothness_loss + 2 * terminal_position_loss\n",
        "        return total_loss\n",
        "\n",
        "    def acceleration_limit_loss(self, pred_velocity, dt=0.1):\n",
        "        # Ensure accelerations remain within feasible limits by penalizing large changes in velocity\n",
        "        approx_acceleration = (pred_velocity[:, :-1, :] - pred_velocity[:, 1:, :]) / dt  # Difference between consecutive velocities\n",
        "        acceleration_norm = torch.norm(approx_acceleration, dim=2)\n",
        "        excess_acceleration = torch.clamp(acceleration_norm - self.max_acceleration, min=0.0)\n",
        "        return torch.mean(excess_acceleration ** 2)\n",
        "\n",
        "\n",
        "    def smoothness_loss(self, pred_position):\n",
        "        # Calculate the difference between consecutive positions\n",
        "        diff = pred_position[:, :-1, :] - pred_position[:, 1:, :]\n",
        "        smoothness = torch.norm(diff, dim=2)\n",
        "        return torch.mean(smoothness)\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, initial_lr=0.001, T_max=10):\n",
        "    criterion = TrajectoryLoss()  # Use the custom loss function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)  # Adam optimizer\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        epoch_train_loss = 0  # Initialize training loss for the epoch\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            outputs = model(batch_x)  # Forward pass\n",
        "            loss = criterion(outputs, batch_y, batch_x[:, 0, :2])  # Compute loss using the custom loss function\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            epoch_train_loss += loss.item()  # Accumulate training loss\n",
        "\n",
        "        # Update learning rate based on the cosine decay schedule\n",
        "        scheduler.step()\n",
        "\n",
        "        # Average training loss for the epoch\n",
        "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)  # Store training loss\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        epoch_val_loss = 0  # Initialize validation loss for the epoch\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "                outputs = model(batch_x)\n",
        "                loss = criterion(outputs, batch_y, batch_x[:, 0, :2])\n",
        "                epoch_val_loss += loss.item()  # Accumulate validation loss\n",
        "\n",
        "            # Average validation loss for the epoch\n",
        "            avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "            val_losses.append(avg_val_loss)  # Store validation loss\n",
        "            print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "# Create Tensor datasets\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_train.shape[2]  # Number of features\n",
        "# hidden_size = 64  # You can adjust this\n",
        "\n",
        "hidden_size = 32  # You can adjust this\n",
        "\n",
        "# input_size = 5\n",
        "# model = TrajectoryLSTM(input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=4).to(\"cuda\")\n",
        "model = TrajectoryLSTM(input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1).to(\"cuda\")\n",
        "\n",
        "\n",
        "# Train the model with cosine decay learning rate\n",
        "\n",
        "\n",
        "train_model(model, train_loader, val_loader, num_epochs=150, initial_lr=1e-2, T_max=50)\n",
        "\n",
        "\n",
        "x = torch.tensor(X_val, dtype=torch.float32).to(\"cuda\")\n",
        "y = torch.tensor(y_val, dtype=torch.float32).to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(x)\n",
        "    cum_disp = torch.cumsum(outputs * 0.1, dim=1)\n",
        "    pred_position = cum_disp + y[:, 0, :2].unsqueeze(1)\n",
        "\n",
        "    distances = torch.norm(pred_position - y[:, :, :2], dim=2)\n",
        "    mean_ade = distances.mean().item()\n",
        "#LSTM\n",
        "print(\"Mean ADE:\", mean_ade)\n",
        "\n",
        "# Choose a random example from the validation dataset\n",
        "example_index = 400 # np.random.randint(0, len(X_val))\n",
        "# 300 produces no movement for x and y\n",
        "# 250 produces an erratic motion for x and y (curvy)\n",
        "\n",
        "past_traj = X_val[example_index]\n",
        "future_traj = y_val[example_index]\n",
        "\n",
        "# Get the predicted future trajectory\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  X = torch.tensor(past_traj[np.newaxis, :, :], dtype=torch.float32).to(\"cuda\")\n",
        "  predicted_vels = model(X)\n",
        "\n",
        "  predicted_vels = predicted_vels[0]\n",
        "  cum_disp = torch.cumsum(predicted_vels * 0.1, dim=1)\n",
        "  future_pred = cum_disp + X[0, -1, :2]\n",
        "  future_pred = future_pred.cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "# Plot the past trajectory, actual future trajectory, and predicted future trajectory\n",
        "#LSTM\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(past_traj[:, 0], past_traj[:, 1], label='Past Trajectory', color='blue')\n",
        "plt.plot(future_traj[:, 0], future_traj[:, 1], label='Actual Future Trajectory', color='red')\n",
        "plt.plot(future_pred[:, 0], future_pred[:, 1], label='Predicted Future Trajectory', color='green')\n",
        "plt.xlabel('X Coordinate')\n",
        "plt.ylabel('Y Coordinate')\n",
        "plt.title(f\"Predicted vs. Actual Future Trajectory (Example {example_index})\")\n",
        "plt.axis('equal')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# print(past_traj[:, :2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mn8-rD1iuoa"
      },
      "source": [
        "# Initial External Actors Implementation - Base Model\n",
        "\n",
        "\n",
        "Vehicle features + pedestrian features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RLGOi6owi17X",
        "outputId": "6d01abb7-ed41-484d-fab2-da0abe63e0c3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "data_path = '/content/drive/MyDrive/CS7643Group/Dataset/training/parsed_data_final_srishti.pkl'\n",
        "with open(data_path, \"rb\") as file:\n",
        "  data = pickle.load(file)\n",
        "\n",
        "print(data.shape)\n",
        "\n",
        "data = data[:, :-1, :]\n",
        "\n",
        "# This is the for all the features\n",
        "# data = data[:, :, 1:]\n",
        "\n",
        "#This is for vehicle features only\n",
        "# data = data[:, :, 1:6]\n",
        "\n",
        "# This is for vehicle + pedestrians only\n",
        "data = data[:, :, 1:8]\n",
        "\n",
        "\n",
        "# This is for vehicle + pedestrians + roadgraphs\n",
        "# another_data = data[:, :, 1:8]\n",
        "# new_data = data[:, :, 10:]\n",
        "# data = np.concatenate((another_data, new_data), axis = 2)\n",
        "\n",
        "\n",
        "print(f\"Shape of full dataset: {data.shape}\")\n",
        "# print(data[0])\n",
        "X_train = data[:int(0.8*len(data)), :80, :]\n",
        "X_val = data[int(0.8*len(data)): , :80, :]\n",
        "\n",
        "y_train = data[:int(0.8*len(data)), 80:, :]\n",
        "y_val = data[int(0.8*len(data)): , 80:, :]\n",
        "\n",
        "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TrajectoryLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1):  # Set default n to 10\n",
        "        super(TrajectoryLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_features * output_timesteps)\n",
        "        self.output_features = output_features\n",
        "        self.output_timesteps = output_timesteps\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_length, input_size)\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        # hn: (num_layers, batch_size, hidden_size)\n",
        "        # want to take the last layer so that you are left with (batch_size, hidden_size)\n",
        "        out = self.fc(hn[-1])\n",
        "        return out.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, 10, input_size)\n",
        "\n",
        "\n",
        "class TrajectoryLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TrajectoryLoss, self).__init__()\n",
        "        self.max_acceleration = 3.0\n",
        "\n",
        "    def forward(self, predictions, targets, last_input_state, dt=0.1):\n",
        "        # Reconstruct trajectory from velocities\n",
        "        pred_velocity = predictions[:, :, :]\n",
        "        cum_disp = torch.cumsum(pred_velocity * dt, dim=1)\n",
        "        # print(pred_velocity.shape)\n",
        "        # (32, 10, 2)\n",
        "        pred_position = cum_disp + last_input_state.unsqueeze(1)\n",
        "        # print(pred_position.shape)\n",
        "\n",
        "        target_position = targets[:, :, :2]\n",
        "        target_velocity = targets[:, :, 2:4]\n",
        "\n",
        "        # Calculate MSE for position\n",
        "\n",
        "        position_loss = F.mse_loss(pred_position, target_position)\n",
        "        velocity_loss = F.mse_loss(pred_velocity, target_velocity)\n",
        "        terminal_position_loss = F.mse_loss(pred_position[:, -1, :], target_position[:, -1, :]) + F.mse_loss(pred_position[:, 0, :], target_position[:, 0, :])\n",
        "        smoothness_loss = self.smoothness_loss(pred_position)\n",
        "\n",
        "        total_loss = position_loss + velocity_loss + smoothness_loss + 2 * terminal_position_loss\n",
        "        return total_loss\n",
        "\n",
        "    def acceleration_limit_loss(self, pred_velocity, dt=0.1):\n",
        "        # Ensure accelerations remain within feasible limits by penalizing large changes in velocity\n",
        "        approx_acceleration = (pred_velocity[:, :-1, :] - pred_velocity[:, 1:, :]) / dt  # Difference between consecutive velocities\n",
        "        acceleration_norm = torch.norm(approx_acceleration, dim=2)\n",
        "        excess_acceleration = torch.clamp(acceleration_norm - self.max_acceleration, min=0.0)\n",
        "        return torch.mean(excess_acceleration ** 2)\n",
        "\n",
        "\n",
        "    def smoothness_loss(self, pred_position):\n",
        "        # Calculate the difference between consecutive positions\n",
        "        diff = pred_position[:, :-1, :] - pred_position[:, 1:, :]\n",
        "        smoothness = torch.norm(diff, dim=2)\n",
        "        return torch.mean(smoothness)\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, initial_lr=0.001, T_max=10):\n",
        "    criterion = TrajectoryLoss()  # Use the custom loss function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)  # Adam optimizer\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        epoch_train_loss = 0  # Initialize training loss for the epoch\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            outputs = model(batch_x)  # Forward pass\n",
        "            loss = criterion(outputs, batch_y, batch_x[:, 0, :2])  # Compute loss using the custom loss function\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            epoch_train_loss += loss.item()  # Accumulate training loss\n",
        "\n",
        "        # Update learning rate based on the cosine decay schedule\n",
        "        scheduler.step()\n",
        "\n",
        "        # Average training loss for the epoch\n",
        "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)  # Store training loss\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        epoch_val_loss = 0  # Initialize validation loss for the epoch\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "                outputs = model(batch_x)\n",
        "                loss = criterion(outputs, batch_y, batch_x[:, 0, :2])\n",
        "                epoch_val_loss += loss.item()  # Accumulate validation loss\n",
        "\n",
        "            # Average validation loss for the epoch\n",
        "            avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "            val_losses.append(avg_val_loss)  # Store validation loss\n",
        "            print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "# Create Tensor datasets\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_train.shape[2]  # Number of features\n",
        "# hidden_size = 64  # You can adjust this\n",
        "\n",
        "hidden_size = 32  # You can adjust this\n",
        "\n",
        "# input_size = 5\n",
        "# model = TrajectoryLSTM(input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=4).to(\"cuda\")\n",
        "model = TrajectoryLSTM(input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1).to(\"cuda\")\n",
        "\n",
        "\n",
        "# Train the model with cosine decay learning rate\n",
        "\n",
        "\n",
        "train_model(model, train_loader, val_loader, num_epochs=150, initial_lr=1e-2, T_max=50)\n",
        "\n",
        "\n",
        "x = torch.tensor(X_val, dtype=torch.float32).to(\"cuda\")\n",
        "y = torch.tensor(y_val, dtype=torch.float32).to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(x)\n",
        "    cum_disp = torch.cumsum(outputs * 0.1, dim=1)\n",
        "    pred_position = cum_disp + y[:, 0, :2].unsqueeze(1)\n",
        "\n",
        "    distances = torch.norm(pred_position - y[:, :, :2], dim=2)\n",
        "    mean_ade = distances.mean().item()\n",
        "#LSTM\n",
        "print(\"Mean ADE:\", mean_ade)\n",
        "\n",
        "# Choose a random example from the validation dataset\n",
        "example_index = 400 # np.random.randint(0, len(X_val))\n",
        "# 300 produces no movement for x and y\n",
        "# 250 produces an erratic motion for x and y (curvy)\n",
        "\n",
        "past_traj = X_val[example_index]\n",
        "future_traj = y_val[example_index]\n",
        "\n",
        "# Get the predicted future trajectory\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  X = torch.tensor(past_traj[np.newaxis, :, :], dtype=torch.float32).to(\"cuda\")\n",
        "  predicted_vels = model(X)\n",
        "\n",
        "  predicted_vels = predicted_vels[0]\n",
        "  cum_disp = torch.cumsum(predicted_vels * 0.1, dim=1)\n",
        "  future_pred = cum_disp + X[0, -1, :2]\n",
        "  future_pred = future_pred.cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "# Plot the past trajectory, actual future trajectory, and predicted future trajectory\n",
        "#LSTM\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(past_traj[:, 0], past_traj[:, 1], label='Past Trajectory', color='blue')\n",
        "plt.plot(future_traj[:, 0], future_traj[:, 1], label='Actual Future Trajectory', color='red')\n",
        "plt.plot(future_pred[:, 0], future_pred[:, 1], label='Predicted Future Trajectory', color='green')\n",
        "plt.xlabel('X Coordinate')\n",
        "plt.ylabel('Y Coordinate')\n",
        "plt.title(f\"Predicted vs. Actual Future Trajectory (Example {example_index})\")\n",
        "plt.axis('equal')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# print(past_traj[:, :2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gbq0eZujEgk"
      },
      "source": [
        "# External Actors - Transformer Architecture\n",
        "\n",
        "vehicle features + pedestrian + cyclist + roadgraph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mV757W7EjIXW",
        "outputId": "f8ff1171-8443-4249-d5d4-e664bb4226eb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "data_path = '/content/drive/MyDrive/CS7643Group/Dataset/training/parsed_data_final_srishti.pkl'\n",
        "with open(data_path, \"rb\") as file:\n",
        "  data = pickle.load(file)\n",
        "\n",
        "print(data.shape)\n",
        "\n",
        "data = data[:, :-1, :]\n",
        "\n",
        "# This is the for all the features\n",
        "data = data[:, :, 1:]\n",
        "\n",
        "#This is for vehicle features only\n",
        "# data = data[:, :, 1:6]\n",
        "\n",
        "# This is for vehicle + pedestrians only\n",
        "# data = data[:, :, 1:8]\n",
        "\n",
        "\n",
        "# This is for vehicle + pedestrians + roadgraphs\n",
        "# another_data = data[:, :, 1:8]\n",
        "# new_data = data[:, :, 10:]\n",
        "# data = np.concatenate((another_data, new_data), axis = 2)\n",
        "\n",
        "\n",
        "print(f\"Shape of full dataset: {data.shape}\")\n",
        "# print(data[0])\n",
        "X_train = data[:int(0.8*len(data)), :80, :]\n",
        "X_val = data[int(0.8*len(data)): , :80, :]\n",
        "\n",
        "y_train = data[:int(0.8*len(data)), 80:, :]\n",
        "y_val = data[int(0.8*len(data)): , 80:, :]\n",
        "\n",
        "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TrajectoryTransformer(nn.Module):\n",
        "    def __init__(self, input_size, model_dim, num_heads, num_layers, output_features=2, output_timesteps=10, max_seq_length=50):\n",
        "        super(TrajectoryTransformer, self).__init__()\n",
        "        self.embedding = nn.Linear(input_size, model_dim)\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(max_seq_length, model_dim))\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=model_dim,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=4 * model_dim,\n",
        "            dropout=0.5 #dropout=0.1\n",
        "        )\n",
        "        self.fc = nn.Linear(model_dim, output_features * output_timesteps)\n",
        "        self.output_features = output_features\n",
        "        self.output_timesteps = output_timesteps\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_length, input_size)\n",
        "        batch_size, seq_length, input_size = x.size()\n",
        "        src = self.embedding(x) + self.positional_encoding[:seq_length]  # Add positional encoding\n",
        "        src = src.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, model_dim)\n",
        "\n",
        "        tgt = torch.zeros_like(src)  # Dummy target (for autoregressive tasks, replace with actual decoder inputs if needed)\n",
        "\n",
        "        # Pass through the Transformer\n",
        "        output = self.transformer(src, tgt)\n",
        "        output = output[-1]  # Take the last sequence position\n",
        "        output = self.fc(output)\n",
        "        return output.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, 10, 2)\n",
        "\n",
        "\n",
        "\n",
        "class TrajectoryLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TrajectoryLoss, self).__init__()\n",
        "        self.max_acceleration = 3.0\n",
        "\n",
        "    def forward(self, predictions, targets, last_input_state, dt=0.1):\n",
        "        # Reconstruct trajectory from velocities\n",
        "        pred_velocity = predictions[:, :, :]\n",
        "        cum_disp = torch.cumsum(pred_velocity * dt, dim=1)\n",
        "        # print(pred_velocity.shape)\n",
        "        # (32, 10, 2)\n",
        "        pred_position = cum_disp + last_input_state.unsqueeze(1)\n",
        "        # print(pred_position.shape)\n",
        "\n",
        "        target_position = targets[:, :, :2]\n",
        "        target_velocity = targets[:, :, 2:4]\n",
        "\n",
        "        # Calculate MSE for position\n",
        "\n",
        "        position_loss = F.mse_loss(pred_position, target_position)\n",
        "        velocity_loss = F.mse_loss(pred_velocity, target_velocity)\n",
        "        terminal_position_loss = F.mse_loss(pred_position[:, -1, :], target_position[:, -1, :]) + F.mse_loss(pred_position[:, 0, :], target_position[:, 0, :])\n",
        "        smoothness_loss = self.smoothness_loss(pred_position)\n",
        "\n",
        "        total_loss = position_loss + velocity_loss + smoothness_loss + 2 * terminal_position_loss\n",
        "        return total_loss\n",
        "\n",
        "    def acceleration_limit_loss(self, pred_velocity, dt=0.1):\n",
        "        # Ensure accelerations remain within feasible limits by penalizing large changes in velocity\n",
        "        approx_acceleration = (pred_velocity[:, :-1, :] - pred_velocity[:, 1:, :]) / dt  # Difference between consecutive velocities\n",
        "        acceleration_norm = torch.norm(approx_acceleration, dim=2)\n",
        "        excess_acceleration = torch.clamp(acceleration_norm - self.max_acceleration, min=0.0)\n",
        "        return torch.mean(excess_acceleration ** 2)\n",
        "\n",
        "\n",
        "    def smoothness_loss(self, pred_position):\n",
        "        # Calculate the difference between consecutive positions\n",
        "        diff = pred_position[:, :-1, :] - pred_position[:, 1:, :]\n",
        "        smoothness = torch.norm(diff, dim=2)\n",
        "        return torch.mean(smoothness)\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, initial_lr=0.001, T_max=10):\n",
        "    criterion = TrajectoryLoss()  # Use the custom loss function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)  # Adam optimizer\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        epoch_train_loss = 0  # Initialize training loss for the epoch\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            outputs = model(batch_x)  # Forward pass\n",
        "            loss = criterion(outputs, batch_y, batch_x[:, 0, :2])  # Compute loss using the custom loss function\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            epoch_train_loss += loss.item()  # Accumulate training loss\n",
        "\n",
        "        # Update learning rate based on the cosine decay schedule\n",
        "        scheduler.step()\n",
        "\n",
        "        # Average training loss for the epoch\n",
        "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)  # Store training loss\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        epoch_val_loss = 0  # Initialize validation loss for the epoch\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "                outputs = model(batch_x)\n",
        "                loss = criterion(outputs, batch_y, batch_x[:, 0, :2])\n",
        "                epoch_val_loss += loss.item()  # Accumulate validation loss\n",
        "\n",
        "            # Average validation loss for the epoch\n",
        "            avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "            val_losses.append(avg_val_loss)  # Store validation loss\n",
        "            print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "# Create Tensor datasets\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "\n",
        "# Create DataLoaders\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_train.shape[2]  # Number of features\n",
        "hidden_size = 64  # You can adjust this\n",
        "\n",
        "# hidden_size = 32  # You can adjust this\n",
        "\n",
        "# input_size = 5\n",
        "# model = TrajectoryLSTM(input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=4).to(\"cuda\")\n",
        "\n",
        "model = TrajectoryLSTM(input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1).to(\"cuda\")\n",
        "\n",
        "\n",
        "# Train the model with cosine decay learning rate\n",
        "\n",
        "\n",
        "train_model(model, train_loader, val_loader, num_epochs=150, initial_lr=1e-2, T_max=50)\n",
        "\n",
        "\n",
        "x = torch.tensor(X_val, dtype=torch.float32).to(\"cuda\")\n",
        "y = torch.tensor(y_val, dtype=torch.float32).to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(x)\n",
        "    cum_disp = torch.cumsum(outputs * 0.1, dim=1)\n",
        "    pred_position = cum_disp + y[:, 0, :2].unsqueeze(1)\n",
        "\n",
        "    distances = torch.norm(pred_position - y[:, :, :2], dim=2)\n",
        "    mean_ade = distances.mean().item()\n",
        "#LSTM\n",
        "print(\"Mean ADE:\", mean_ade)\n",
        "\n",
        "\n",
        "\n",
        "# Choose a random example from the validation dataset\n",
        "example_index = 400 # np.random.randint(0, len(X_val))\n",
        "# 300 produces no movement for x and y\n",
        "# 250 produces an erratic motion for x and y (curvy)\n",
        "# 492 -> no motion in x direction, only movement in y-direction\n",
        "\n",
        "past_traj = X_val[example_index]\n",
        "future_traj = y_val[example_index]\n",
        "\n",
        "# Get the predicted future trajectory\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  X = torch.tensor(past_traj[np.newaxis, :, :], dtype=torch.float32).to(\"cuda\")\n",
        "  predicted_vels = model(X)\n",
        "\n",
        "  predicted_vels = predicted_vels[0]\n",
        "  cum_disp = torch.cumsum(predicted_vels * 0.1, dim=1)\n",
        "  future_pred = cum_disp + X[0, -1, :2]\n",
        "  future_pred = future_pred.cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "# Plot the past trajectory, actual future trajectory, and predicted future trajectory\n",
        "#LSTM\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(past_traj[:, 0], past_traj[:, 1], label='Past Trajectory', color='blue')\n",
        "plt.plot(future_traj[:, 0], future_traj[:, 1], label='Actual Future Trajectory', color='red')\n",
        "plt.plot(future_pred[:, 0], future_pred[:, 1], label='Predicted Future Trajectory', color='green')\n",
        "plt.xlabel('X Coordinate')\n",
        "plt.ylabel('Y Coordinate')\n",
        "plt.title(f\"Predicted vs. Actual Future Trajectory (Example {example_index})\")\n",
        "plt.axis('equal')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# print(past_traj[:, :2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmO5zNIgjlxz"
      },
      "source": [
        "# External Actors - Transformer Architecture\n",
        "\n",
        "vehicle features + pedestrian + roadgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "st4EpKTuj1x3",
        "outputId": "f1a8f2c0-f5ce-49d1-dcea-40f071ac131c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "data_path = '/content/drive/MyDrive/CS7643Group/Dataset/training/parsed_data_final_srishti.pkl'\n",
        "with open(data_path, \"rb\") as file:\n",
        "  data = pickle.load(file)\n",
        "\n",
        "print(data.shape)\n",
        "\n",
        "data = data[:, :-1, :]\n",
        "\n",
        "# This is the for all the features\n",
        "# data = data[:, :, 1:]\n",
        "\n",
        "#This is for vehicle features only\n",
        "# data = data[:, :, 1:6]\n",
        "\n",
        "# This is for vehicle + pedestrians only\n",
        "# data = data[:, :, 1:8]\n",
        "\n",
        "\n",
        "# This is for vehicle + pedestrians + roadgraphs\n",
        "another_data = data[:, :, 1:8]\n",
        "new_data = data[:, :, 10:]\n",
        "data = np.concatenate((another_data, new_data), axis = 2)\n",
        "\n",
        "\n",
        "print(f\"Shape of full dataset: {data.shape}\")\n",
        "# print(data[0])\n",
        "X_train = data[:int(0.8*len(data)), :80, :]\n",
        "X_val = data[int(0.8*len(data)): , :80, :]\n",
        "\n",
        "y_train = data[:int(0.8*len(data)), 80:, :]\n",
        "y_val = data[int(0.8*len(data)): , 80:, :]\n",
        "\n",
        "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TrajectoryTransformer(nn.Module):\n",
        "    def __init__(self, input_size, model_dim, num_heads, num_layers, output_features=2, output_timesteps=10, max_seq_length=50):\n",
        "        super(TrajectoryTransformer, self).__init__()\n",
        "        self.embedding = nn.Linear(input_size, model_dim)\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(max_seq_length, model_dim))\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=model_dim,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=4 * model_dim,\n",
        "            dropout=0.5 #dropout=0.1\n",
        "        )\n",
        "        self.fc = nn.Linear(model_dim, output_features * output_timesteps)\n",
        "        self.output_features = output_features\n",
        "        self.output_timesteps = output_timesteps\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_length, input_size)\n",
        "        batch_size, seq_length, input_size = x.size()\n",
        "        src = self.embedding(x) + self.positional_encoding[:seq_length]  # Add positional encoding\n",
        "        src = src.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, model_dim)\n",
        "\n",
        "        tgt = torch.zeros_like(src)  # Dummy target (for autoregressive tasks, replace with actual decoder inputs if needed)\n",
        "\n",
        "        # Pass through the Transformer\n",
        "        output = self.transformer(src, tgt)\n",
        "        output = output[-1]  # Take the last sequence position\n",
        "        output = self.fc(output)\n",
        "        return output.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, 10, 2)\n",
        "\n",
        "\n",
        "\n",
        "class TrajectoryLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TrajectoryLoss, self).__init__()\n",
        "        self.max_acceleration = 3.0\n",
        "\n",
        "    def forward(self, predictions, targets, last_input_state, dt=0.1):\n",
        "        # Reconstruct trajectory from velocities\n",
        "        pred_velocity = predictions[:, :, :]\n",
        "        cum_disp = torch.cumsum(pred_velocity * dt, dim=1)\n",
        "        # print(pred_velocity.shape)\n",
        "        # (32, 10, 2)\n",
        "        pred_position = cum_disp + last_input_state.unsqueeze(1)\n",
        "        # print(pred_position.shape)\n",
        "\n",
        "        target_position = targets[:, :, :2]\n",
        "        target_velocity = targets[:, :, 2:4]\n",
        "\n",
        "        # Calculate MSE for position\n",
        "\n",
        "        position_loss = F.mse_loss(pred_position, target_position)\n",
        "        velocity_loss = F.mse_loss(pred_velocity, target_velocity)\n",
        "        terminal_position_loss = F.mse_loss(pred_position[:, -1, :], target_position[:, -1, :]) + F.mse_loss(pred_position[:, 0, :], target_position[:, 0, :])\n",
        "        smoothness_loss = self.smoothness_loss(pred_position)\n",
        "\n",
        "        total_loss = position_loss + velocity_loss + smoothness_loss + 2 * terminal_position_loss\n",
        "        return total_loss\n",
        "\n",
        "    def acceleration_limit_loss(self, pred_velocity, dt=0.1):\n",
        "        # Ensure accelerations remain within feasible limits by penalizing large changes in velocity\n",
        "        approx_acceleration = (pred_velocity[:, :-1, :] - pred_velocity[:, 1:, :]) / dt  # Difference between consecutive velocities\n",
        "        acceleration_norm = torch.norm(approx_acceleration, dim=2)\n",
        "        excess_acceleration = torch.clamp(acceleration_norm - self.max_acceleration, min=0.0)\n",
        "        return torch.mean(excess_acceleration ** 2)\n",
        "\n",
        "\n",
        "    def smoothness_loss(self, pred_position):\n",
        "        # Calculate the difference between consecutive positions\n",
        "        diff = pred_position[:, :-1, :] - pred_position[:, 1:, :]\n",
        "        smoothness = torch.norm(diff, dim=2)\n",
        "        return torch.mean(smoothness)\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, initial_lr=0.001, T_max=10):\n",
        "    criterion = TrajectoryLoss()  # Use the custom loss function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)  # Adam optimizer\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        epoch_train_loss = 0  # Initialize training loss for the epoch\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            outputs = model(batch_x)  # Forward pass\n",
        "            loss = criterion(outputs, batch_y, batch_x[:, 0, :2])  # Compute loss using the custom loss function\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            epoch_train_loss += loss.item()  # Accumulate training loss\n",
        "\n",
        "        # Update learning rate based on the cosine decay schedule\n",
        "        scheduler.step()\n",
        "\n",
        "        # Average training loss for the epoch\n",
        "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)  # Store training loss\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        epoch_val_loss = 0  # Initialize validation loss for the epoch\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "                outputs = model(batch_x)\n",
        "                loss = criterion(outputs, batch_y, batch_x[:, 0, :2])\n",
        "                epoch_val_loss += loss.item()  # Accumulate validation loss\n",
        "\n",
        "            # Average validation loss for the epoch\n",
        "            avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "            val_losses.append(avg_val_loss)  # Store validation loss\n",
        "            print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "# Create Tensor datasets\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "\n",
        "# Create DataLoaders\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_train.shape[2]  # Number of features\n",
        "hidden_size = 64  # You can adjust this\n",
        "\n",
        "# hidden_size = 32  # You can adjust this\n",
        "\n",
        "# input_size = 5\n",
        "# model = TrajectoryLSTM(input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=4).to(\"cuda\")\n",
        "\n",
        "model = TrajectoryLSTM(input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1).to(\"cuda\")\n",
        "\n",
        "\n",
        "# Train the model with cosine decay learning rate\n",
        "\n",
        "\n",
        "train_model(model, train_loader, val_loader, num_epochs=150, initial_lr=1e-2, T_max=50)\n",
        "\n",
        "\n",
        "x = torch.tensor(X_val, dtype=torch.float32).to(\"cuda\")\n",
        "y = torch.tensor(y_val, dtype=torch.float32).to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(x)\n",
        "    cum_disp = torch.cumsum(outputs * 0.1, dim=1)\n",
        "    pred_position = cum_disp + y[:, 0, :2].unsqueeze(1)\n",
        "\n",
        "    distances = torch.norm(pred_position - y[:, :, :2], dim=2)\n",
        "    mean_ade = distances.mean().item()\n",
        "#LSTM\n",
        "print(\"Mean ADE:\", mean_ade)\n",
        "\n",
        "\n",
        "\n",
        "# Choose a random example from the validation dataset\n",
        "example_index = 400 # np.random.randint(0, len(X_val))\n",
        "# 300 produces no movement for x and y\n",
        "# 250 produces an erratic motion for x and y (curvy)\n",
        "# 492 -> no motion in x direction, only movement in y-direction\n",
        "\n",
        "past_traj = X_val[example_index]\n",
        "future_traj = y_val[example_index]\n",
        "\n",
        "# Get the predicted future trajectory\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  X = torch.tensor(past_traj[np.newaxis, :, :], dtype=torch.float32).to(\"cuda\")\n",
        "  predicted_vels = model(X)\n",
        "\n",
        "  predicted_vels = predicted_vels[0]\n",
        "  cum_disp = torch.cumsum(predicted_vels * 0.1, dim=1)\n",
        "  future_pred = cum_disp + X[0, -1, :2]\n",
        "  future_pred = future_pred.cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "# Plot the past trajectory, actual future trajectory, and predicted future trajectory\n",
        "#LSTM\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(past_traj[:, 0], past_traj[:, 1], label='Past Trajectory', color='blue')\n",
        "plt.plot(future_traj[:, 0], future_traj[:, 1], label='Actual Future Trajectory', color='red')\n",
        "plt.plot(future_pred[:, 0], future_pred[:, 1], label='Predicted Future Trajectory', color='green')\n",
        "plt.xlabel('X Coordinate')\n",
        "plt.ylabel('Y Coordinate')\n",
        "plt.title(f\"Predicted vs. Actual Future Trajectory (Example {example_index})\")\n",
        "plt.axis('equal')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# print(past_traj[:, :2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fY8mwf3kOz6"
      },
      "source": [
        "# External Actors - Transformer Architecture\n",
        "\n",
        "vehicle features + pedestrian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_jsVQjLZkLSV",
        "outputId": "ed30f41c-1202-436a-9c86-c3ed5650f364"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "data_path = '/content/drive/MyDrive/CS7643Group/Dataset/training/parsed_data_final_srishti.pkl'\n",
        "with open(data_path, \"rb\") as file:\n",
        "  data = pickle.load(file)\n",
        "\n",
        "print(data.shape)\n",
        "\n",
        "data = data[:, :-1, :]\n",
        "\n",
        "# This is the for all the features\n",
        "# data = data[:, :, 1:]\n",
        "\n",
        "#This is for vehicle features only\n",
        "# data = data[:, :, 1:6]\n",
        "\n",
        "# This is for vehicle + pedestrians only\n",
        "data = data[:, :, 1:8]\n",
        "\n",
        "\n",
        "# This is for vehicle + pedestrians + roadgraphs\n",
        "# another_data = data[:, :, 1:8]\n",
        "# new_data = data[:, :, 10:]\n",
        "# data = np.concatenate((another_data, new_data), axis = 2)\n",
        "\n",
        "\n",
        "print(f\"Shape of full dataset: {data.shape}\")\n",
        "# print(data[0])\n",
        "X_train = data[:int(0.8*len(data)), :80, :]\n",
        "X_val = data[int(0.8*len(data)): , :80, :]\n",
        "\n",
        "y_train = data[:int(0.8*len(data)), 80:, :]\n",
        "y_val = data[int(0.8*len(data)): , 80:, :]\n",
        "\n",
        "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TrajectoryTransformer(nn.Module):\n",
        "    def __init__(self, input_size, model_dim, num_heads, num_layers, output_features=2, output_timesteps=10, max_seq_length=50):\n",
        "        super(TrajectoryTransformer, self).__init__()\n",
        "        self.embedding = nn.Linear(input_size, model_dim)\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(max_seq_length, model_dim))\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=model_dim,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=4 * model_dim,\n",
        "            dropout=0.5 #dropout=0.1\n",
        "        )\n",
        "        self.fc = nn.Linear(model_dim, output_features * output_timesteps)\n",
        "        self.output_features = output_features\n",
        "        self.output_timesteps = output_timesteps\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_length, input_size)\n",
        "        batch_size, seq_length, input_size = x.size()\n",
        "        src = self.embedding(x) + self.positional_encoding[:seq_length]  # Add positional encoding\n",
        "        src = src.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, model_dim)\n",
        "\n",
        "        tgt = torch.zeros_like(src)  # Dummy target (for autoregressive tasks, replace with actual decoder inputs if needed)\n",
        "\n",
        "        # Pass through the Transformer\n",
        "        output = self.transformer(src, tgt)\n",
        "        output = output[-1]  # Take the last sequence position\n",
        "        output = self.fc(output)\n",
        "        return output.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, 10, 2)\n",
        "\n",
        "\n",
        "\n",
        "class TrajectoryLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TrajectoryLoss, self).__init__()\n",
        "        self.max_acceleration = 3.0\n",
        "\n",
        "    def forward(self, predictions, targets, last_input_state, dt=0.1):\n",
        "        # Reconstruct trajectory from velocities\n",
        "        pred_velocity = predictions[:, :, :]\n",
        "        cum_disp = torch.cumsum(pred_velocity * dt, dim=1)\n",
        "        # print(pred_velocity.shape)\n",
        "        # (32, 10, 2)\n",
        "        pred_position = cum_disp + last_input_state.unsqueeze(1)\n",
        "        # print(pred_position.shape)\n",
        "\n",
        "        target_position = targets[:, :, :2]\n",
        "        target_velocity = targets[:, :, 2:4]\n",
        "\n",
        "        # Calculate MSE for position\n",
        "\n",
        "        position_loss = F.mse_loss(pred_position, target_position)\n",
        "        velocity_loss = F.mse_loss(pred_velocity, target_velocity)\n",
        "        terminal_position_loss = F.mse_loss(pred_position[:, -1, :], target_position[:, -1, :]) + F.mse_loss(pred_position[:, 0, :], target_position[:, 0, :])\n",
        "        smoothness_loss = self.smoothness_loss(pred_position)\n",
        "\n",
        "        total_loss = position_loss + velocity_loss + smoothness_loss + 2 * terminal_position_loss\n",
        "        return total_loss\n",
        "\n",
        "    def acceleration_limit_loss(self, pred_velocity, dt=0.1):\n",
        "        # Ensure accelerations remain within feasible limits by penalizing large changes in velocity\n",
        "        approx_acceleration = (pred_velocity[:, :-1, :] - pred_velocity[:, 1:, :]) / dt  # Difference between consecutive velocities\n",
        "        acceleration_norm = torch.norm(approx_acceleration, dim=2)\n",
        "        excess_acceleration = torch.clamp(acceleration_norm - self.max_acceleration, min=0.0)\n",
        "        return torch.mean(excess_acceleration ** 2)\n",
        "\n",
        "\n",
        "    def smoothness_loss(self, pred_position):\n",
        "        # Calculate the difference between consecutive positions\n",
        "        diff = pred_position[:, :-1, :] - pred_position[:, 1:, :]\n",
        "        smoothness = torch.norm(diff, dim=2)\n",
        "        return torch.mean(smoothness)\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, initial_lr=0.001, T_max=10):\n",
        "    criterion = TrajectoryLoss()  # Use the custom loss function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)  # Adam optimizer\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        epoch_train_loss = 0  # Initialize training loss for the epoch\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            outputs = model(batch_x)  # Forward pass\n",
        "            loss = criterion(outputs, batch_y, batch_x[:, 0, :2])  # Compute loss using the custom loss function\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            epoch_train_loss += loss.item()  # Accumulate training loss\n",
        "\n",
        "        # Update learning rate based on the cosine decay schedule\n",
        "        scheduler.step()\n",
        "\n",
        "        # Average training loss for the epoch\n",
        "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)  # Store training loss\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        epoch_val_loss = 0  # Initialize validation loss for the epoch\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "                outputs = model(batch_x)\n",
        "                loss = criterion(outputs, batch_y, batch_x[:, 0, :2])\n",
        "                epoch_val_loss += loss.item()  # Accumulate validation loss\n",
        "\n",
        "            # Average validation loss for the epoch\n",
        "            avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "            val_losses.append(avg_val_loss)  # Store validation loss\n",
        "            print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "# Create Tensor datasets\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "\n",
        "# Create DataLoaders\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_train.shape[2]  # Number of features\n",
        "hidden_size = 64  # You can adjust this\n",
        "\n",
        "# hidden_size = 32  # You can adjust this\n",
        "\n",
        "# input_size = 5\n",
        "# model = TrajectoryLSTM(input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=4).to(\"cuda\")\n",
        "\n",
        "model = TrajectoryLSTM(input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1).to(\"cuda\")\n",
        "\n",
        "\n",
        "# Train the model with cosine decay learning rate\n",
        "\n",
        "\n",
        "train_model(model, train_loader, val_loader, num_epochs=150, initial_lr=1e-2, T_max=50)\n",
        "\n",
        "\n",
        "x = torch.tensor(X_val, dtype=torch.float32).to(\"cuda\")\n",
        "y = torch.tensor(y_val, dtype=torch.float32).to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(x)\n",
        "    cum_disp = torch.cumsum(outputs * 0.1, dim=1)\n",
        "    pred_position = cum_disp + y[:, 0, :2].unsqueeze(1)\n",
        "\n",
        "    distances = torch.norm(pred_position - y[:, :, :2], dim=2)\n",
        "    mean_ade = distances.mean().item()\n",
        "#LSTM\n",
        "print(\"Mean ADE:\", mean_ade)\n",
        "\n",
        "\n",
        "\n",
        "# Choose a random example from the validation dataset\n",
        "example_index = 400 # np.random.randint(0, len(X_val))\n",
        "# 300 produces no movement for x and y\n",
        "# 250 produces an erratic motion for x and y (curvy)\n",
        "# 492 -> no motion in x direction, only movement in y-direction\n",
        "\n",
        "past_traj = X_val[example_index]\n",
        "future_traj = y_val[example_index]\n",
        "\n",
        "# Get the predicted future trajectory\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  X = torch.tensor(past_traj[np.newaxis, :, :], dtype=torch.float32).to(\"cuda\")\n",
        "  predicted_vels = model(X)\n",
        "\n",
        "  predicted_vels = predicted_vels[0]\n",
        "  cum_disp = torch.cumsum(predicted_vels * 0.1, dim=1)\n",
        "  future_pred = cum_disp + X[0, -1, :2]\n",
        "  future_pred = future_pred.cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "# Plot the past trajectory, actual future trajectory, and predicted future trajectory\n",
        "#LSTM\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(past_traj[:, 0], past_traj[:, 1], label='Past Trajectory', color='blue')\n",
        "plt.plot(future_traj[:, 0], future_traj[:, 1], label='Actual Future Trajectory', color='red')\n",
        "plt.plot(future_pred[:, 0], future_pred[:, 1], label='Predicted Future Trajectory', color='green')\n",
        "plt.xlabel('X Coordinate')\n",
        "plt.ylabel('Y Coordinate')\n",
        "plt.title(f\"Predicted vs. Actual Future Trajectory (Example {example_index})\")\n",
        "plt.axis('equal')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# print(past_traj[:, :2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmrGc8-SlMxD"
      },
      "source": [
        "# External Actors Implementation - Positional Encoding, Dropout, Single Headed Attention\n",
        "\n",
        "Vehicle features + pedestrian + cyclist + roadgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zD7omKwplc8_",
        "outputId": "ac9f87e9-cb7a-445c-a4e6-2a32b384b305"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "data_path = '/content/drive/MyDrive/CS7643Group/Dataset/training/parsed_data_final_srishti.pkl'\n",
        "with open(data_path, \"rb\") as file:\n",
        "  data = pickle.load(file)\n",
        "\n",
        "print(data.shape)\n",
        "\n",
        "data = data[:, :-1, :]\n",
        "\n",
        "# This is the for all the features\n",
        "data = data[:, :, 1:]\n",
        "\n",
        "#This is for vehicle features only\n",
        "# data = data[:, :, 1:6]\n",
        "\n",
        "# This is for vehicle + pedestrians only\n",
        "# data = data[:, :, 1:8]\n",
        "\n",
        "\n",
        "# This is for vehicle + pedestrians + roadgraphs\n",
        "# another_data = data[:, :, 1:8]\n",
        "# new_data = data[:, :, 10:]\n",
        "# data = np.concatenate((another_data, new_data), axis = 2)\n",
        "\n",
        "\n",
        "print(f\"Shape of full dataset: {data.shape}\")\n",
        "# print(data[0])\n",
        "X_train = data[:int(0.8*len(data)), :80, :]\n",
        "X_val = data[int(0.8*len(data)): , :80, :]\n",
        "\n",
        "y_train = data[:int(0.8*len(data)), 80:, :]\n",
        "y_val = data[int(0.8*len(data)): , 80:, :]\n",
        "\n",
        "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
        "\n",
        "\n",
        "#Kavya and Srishti\n",
        "# External Actors added\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TrajectoryLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1, dropout=0.5):\n",
        "        super(TrajectoryLSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_features = output_features\n",
        "        self.output_timesteps = output_timesteps\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)  # Dropout layer\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Linear(hidden_size, 1)  # To compute attention scores\n",
        "        self.softmax = nn.Softmax(dim=1)  # For normalizing attention scores\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, output_features * output_timesteps)\n",
        "        )\n",
        "\n",
        "    def positional_encoding(self, seq_length, dim):\n",
        "        # Create the positional encoding matrix\n",
        "        pos = torch.arange(seq_length, dtype=torch.float32).unsqueeze(1)\n",
        "        i = torch.arange(dim, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        # Compute sinusoidal functions\n",
        "        angle_rates = 1 / (10000 ** (2 * (i // 2) / dim))\n",
        "        angle_rads = pos * angle_rates\n",
        "\n",
        "        # Apply sin to even indices and cos to odd indices\n",
        "        pos_enc = torch.zeros((seq_length, dim))\n",
        "        pos_enc[:, 0::2] = torch.sin(angle_rads[:, 0::2])  # sin for even indices\n",
        "        pos_enc[:, 1::2] = torch.cos(angle_rads[:, 1::2])  # cos for odd indices\n",
        "\n",
        "        return pos_enc\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "\n",
        "        # Add positional encoding\n",
        "        pos_enc = self.positional_encoding(seq_length, self.input_size).to(x.device)\n",
        "        x = x + pos_enc.unsqueeze(0)  # Broadcast to batch size\n",
        "\n",
        "        # LSTM forward pass\n",
        "        lstm_out, (hn, _) = self.lstm(x)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn_scores = self.attention(lstm_out)  # (batch_size, seq_length, 1)\n",
        "        attn_weights = self.softmax(attn_scores)  # Normalize to get weights\n",
        "\n",
        "        # Apply attention weights to LSTM outputs\n",
        "        weighted_output = lstm_out * attn_weights  # (batch_size, seq_length, hidden_size)\n",
        "        context_vector = torch.sum(weighted_output, dim=1)  # Sum across seq_length\n",
        "\n",
        "        # Apply dropout and fully connected layers\n",
        "        context_vector = self.dropout(context_vector)\n",
        "        out = self.fc(context_vector)  # (batch_size, output_timesteps * output_features)\n",
        "        return out.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, output_timesteps, output_features)\n",
        "\n",
        "\n",
        "class TrajectoryLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TrajectoryLoss, self).__init__()\n",
        "        self.max_acceleration = 3.0\n",
        "\n",
        "    def forward(self, predictions, targets, last_input_state, dt=0.2):\n",
        "        # Reconstruct trajectory from velocities\n",
        "        pred_velocity = predictions[:, :, :]\n",
        "        cum_disp = torch.cumsum(pred_velocity * dt, dim=1)\n",
        "        # print(pred_velocity.shape)\n",
        "        # (32, 10, 2)\n",
        "\n",
        "        pred_position = cum_disp + last_input_state.unsqueeze(1)\n",
        "\n",
        "        # print(pred_position.shape)\n",
        "\n",
        "        target_position = targets[:, :, :2]\n",
        "        target_velocity = targets[:, :, 2:4]\n",
        "\n",
        "        # Calculate MSE for position\n",
        "\n",
        "        position_loss = F.mse_loss(pred_position, target_position)\n",
        "\n",
        "        velocity_loss = F.mse_loss(pred_velocity, target_velocity)\n",
        "        terminal_position_loss = F.mse_loss(pred_position[:, -1, :], target_position[:, -1, :]) + F.mse_loss(pred_position[:, 0, :], target_position[:, 0, :])\n",
        "        smoothness_loss = self.smoothness_loss(pred_position)\n",
        "\n",
        "        # total_loss = position_loss + velocity_loss + smoothness_loss + 2 * terminal_position_loss\n",
        "        total_loss = 2 * position_loss + velocity_loss + 0.5 * smoothness_loss + 2 * terminal_position_loss\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def acceleration_limit_loss(self, pred_velocity, dt=0.1):\n",
        "        # Ensure accelerations remain within feasible limits by penalizing large changes in velocity\n",
        "        approx_acceleration = (pred_velocity[:, :-1, :] - pred_velocity[:, 1:, :]) / dt  # Difference between consecutive velocities\n",
        "        acceleration_norm = torch.norm(approx_acceleration, dim=2)\n",
        "        excess_acceleration = torch.clamp(acceleration_norm - self.max_acceleration, min=0.0)\n",
        "        return torch.mean(excess_acceleration ** 2)\n",
        "\n",
        "\n",
        "    def smoothness_loss(self, pred_position):\n",
        "        # Calculate the difference between consecutive positions\n",
        "        diff = pred_position[:, :-1, :] - pred_position[:, 1:, :]\n",
        "        smoothness = torch.norm(diff, dim=2)\n",
        "        return torch.mean(smoothness)\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, initial_lr=0.001, T_max=10):\n",
        "    criterion = TrajectoryLoss()  # Use the custom loss function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)  # Adam optimizer\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        epoch_train_loss = 0  # Initialize training loss for the epoch\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            outputs = model(batch_x)  # Forward pass\n",
        "            loss = criterion(outputs, batch_y, batch_x[:, 0, :2])  # Compute loss using the custom loss function\n",
        "            loss.backward()  # Backward pass\n",
        "            # added this to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=3)\n",
        "\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            epoch_train_loss += loss.item()  # Accumulate training loss\n",
        "\n",
        "        # Update learning rate based on the cosine decay schedule\n",
        "        scheduler.step()\n",
        "\n",
        "        # Average training loss for the epoch\n",
        "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)  # Store training loss\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        epoch_val_loss = 0  # Initialize validation loss for the epoch\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "                outputs = model(batch_x)\n",
        "                loss = criterion(outputs, batch_y, batch_x[:, 0, :2])\n",
        "                epoch_val_loss += loss.item()  # Accumulate validation loss\n",
        "\n",
        "            # Average validation loss for the epoch\n",
        "            avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "            val_losses.append(avg_val_loss)  # Store validation loss\n",
        "            print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "# Create Tensor datasets\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_train.shape[2]  # Number of features\n",
        "hidden_size = 128  # You can adjust this\n",
        "# input_size = 5\n",
        "model = TrajectoryLSTM(input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1, dropout=0.5).to(\"cuda\")\n",
        "\n",
        "# Train the model with cosine decay learning rate\n",
        "train_model(model, train_loader, val_loader, num_epochs=150, initial_lr=1e-2, T_max=50)\n",
        "\n",
        "def plot_loss_curves(train_losses, val_losses):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(train_losses, label='Training Loss', color='blue')\n",
        "    plt.plot(val_losses, label='Validation Loss', color='orange')\n",
        "    plt.title('Training and Validation Loss Curves')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_curves(train_losses, val_losses)\n",
        "\n",
        "x = torch.tensor(X_val, dtype=torch.float32).to(\"cuda\")\n",
        "y = torch.tensor(y_val, dtype=torch.float32).to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(x)\n",
        "    cum_disp = torch.cumsum(outputs * 0.1, dim=1)\n",
        "    pred_position = cum_disp + y[:, 0, :2].unsqueeze(1)\n",
        "\n",
        "    distances = torch.norm(pred_position - y[:, :, :2], dim=2)\n",
        "    mean_ade = distances.mean().item()\n",
        "#LSTM\n",
        "print(\"Mean ADE:\", mean_ade)\n",
        "\n",
        "# Choose a random example from the validation dataset\n",
        "example_index = 400 # np.random.randint(0, len(X_val))\n",
        "past_traj = X_val[example_index]\n",
        "future_traj = y_val[example_index]\n",
        "\n",
        "# Get the predicted future trajectory\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  X = torch.tensor(past_traj[np.newaxis, :, :], dtype=torch.float32).to(\"cuda\")\n",
        "  predicted_vels = model(X)\n",
        "\n",
        "  predicted_vels = predicted_vels[0]\n",
        "  cum_disp = torch.cumsum(predicted_vels * 0.1, dim=1)\n",
        "  future_pred = cum_disp + X[0, -1, :2]\n",
        "  future_pred = future_pred.cpu().numpy()\n",
        "\n",
        "# print(past_traj[:, :2])\n",
        "\n",
        "\n",
        "# Plot the past trajectory, actual future trajectory, and predicted future trajectory\n",
        "#LSTM\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(past_traj[:, 0], past_traj[:, 1], label='Past Trajectory', color='blue')\n",
        "plt.plot(future_traj[:, 0], future_traj[:, 1], label='Actual Future Trajectory', color='red')\n",
        "plt.plot(future_pred[:, 0], future_pred[:, 1], label='Predicted Future Trajectory', color='green')\n",
        "plt.xlabel('X Coordinate')\n",
        "plt.ylabel('Y Coordinate')\n",
        "plt.title(f\"Predicted vs. Actual Future Trajectory (Example {example_index})\")\n",
        "plt.axis('equal')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UGhOPA4l_o-"
      },
      "source": [
        "# External Actors Implementation - Positional Encoding, Dropout, Single Headed Attention\n",
        "\n",
        "Vehicle features + pedestrian + roadgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L59kwvDEmBME",
        "outputId": "aa82a531-d9ce-414a-a5e3-acb64c84c9ab"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "data_path = '/content/drive/MyDrive/CS7643Group/Dataset/training/parsed_data_final_srishti.pkl'\n",
        "with open(data_path, \"rb\") as file:\n",
        "  data = pickle.load(file)\n",
        "\n",
        "print(data.shape)\n",
        "\n",
        "data = data[:, :-1, :]\n",
        "\n",
        "# This is the for all the features\n",
        "# data = data[:, :, 1:]\n",
        "\n",
        "#This is for vehicle features only\n",
        "# data = data[:, :, 1:6]\n",
        "\n",
        "# This is for vehicle + pedestrians only\n",
        "# data = data[:, :, 1:8]\n",
        "\n",
        "\n",
        "# This is for vehicle + pedestrians + roadgraphs\n",
        "another_data = data[:, :, 1:8]\n",
        "new_data = data[:, :, 10:]\n",
        "data = np.concatenate((another_data, new_data), axis = 2)\n",
        "\n",
        "\n",
        "print(f\"Shape of full dataset: {data.shape}\")\n",
        "# print(data[0])\n",
        "X_train = data[:int(0.8*len(data)), :80, :]\n",
        "X_val = data[int(0.8*len(data)): , :80, :]\n",
        "\n",
        "y_train = data[:int(0.8*len(data)), 80:, :]\n",
        "y_val = data[int(0.8*len(data)): , 80:, :]\n",
        "\n",
        "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Kavya and Srishti\n",
        "# External Actors added\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TrajectoryLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1, dropout=0.5):\n",
        "        super(TrajectoryLSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_features = output_features\n",
        "        self.output_timesteps = output_timesteps\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)  # Dropout layer\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Linear(hidden_size, 1)  # To compute attention scores\n",
        "        self.softmax = nn.Softmax(dim=1)  # For normalizing attention scores\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, output_features * output_timesteps)\n",
        "        )\n",
        "\n",
        "    def positional_encoding(self, seq_length, dim):\n",
        "        # Create the positional encoding matrix\n",
        "        pos = torch.arange(seq_length, dtype=torch.float32).unsqueeze(1)\n",
        "        i = torch.arange(dim, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        # Compute sinusoidal functions\n",
        "        angle_rates = 1 / (10000 ** (2 * (i // 2) / dim))\n",
        "        angle_rads = pos * angle_rates\n",
        "\n",
        "        # Apply sin to even indices and cos to odd indices\n",
        "        pos_enc = torch.zeros((seq_length, dim))\n",
        "        pos_enc[:, 0::2] = torch.sin(angle_rads[:, 0::2])  # sin for even indices\n",
        "        pos_enc[:, 1::2] = torch.cos(angle_rads[:, 1::2])  # cos for odd indices\n",
        "\n",
        "        return pos_enc\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "\n",
        "        # Add positional encoding\n",
        "        pos_enc = self.positional_encoding(seq_length, self.input_size).to(x.device)\n",
        "        x = x + pos_enc.unsqueeze(0)  # Broadcast to batch size\n",
        "\n",
        "        # LSTM forward pass\n",
        "        lstm_out, (hn, _) = self.lstm(x)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn_scores = self.attention(lstm_out)  # (batch_size, seq_length, 1)\n",
        "        attn_weights = self.softmax(attn_scores)  # Normalize to get weights\n",
        "\n",
        "        # Apply attention weights to LSTM outputs\n",
        "        weighted_output = lstm_out * attn_weights  # (batch_size, seq_length, hidden_size)\n",
        "        context_vector = torch.sum(weighted_output, dim=1)  # Sum across seq_length\n",
        "\n",
        "        # Apply dropout and fully connected layers\n",
        "        context_vector = self.dropout(context_vector)\n",
        "        out = self.fc(context_vector)  # (batch_size, output_timesteps * output_features)\n",
        "        return out.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, output_timesteps, output_features)\n",
        "\n",
        "\n",
        "class TrajectoryLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TrajectoryLoss, self).__init__()\n",
        "        self.max_acceleration = 3.0\n",
        "\n",
        "    def forward(self, predictions, targets, last_input_state, dt=0.2):\n",
        "        # Reconstruct trajectory from velocities\n",
        "        pred_velocity = predictions[:, :, :]\n",
        "        cum_disp = torch.cumsum(pred_velocity * dt, dim=1)\n",
        "        # print(pred_velocity.shape)\n",
        "        # (32, 10, 2)\n",
        "\n",
        "        pred_position = cum_disp + last_input_state.unsqueeze(1)\n",
        "\n",
        "        # print(pred_position.shape)\n",
        "\n",
        "        target_position = targets[:, :, :2]\n",
        "        target_velocity = targets[:, :, 2:4]\n",
        "\n",
        "        # Calculate MSE for position\n",
        "\n",
        "        position_loss = F.mse_loss(pred_position, target_position)\n",
        "\n",
        "        velocity_loss = F.mse_loss(pred_velocity, target_velocity)\n",
        "        terminal_position_loss = F.mse_loss(pred_position[:, -1, :], target_position[:, -1, :]) + F.mse_loss(pred_position[:, 0, :], target_position[:, 0, :])\n",
        "        smoothness_loss = self.smoothness_loss(pred_position)\n",
        "\n",
        "        # total_loss = position_loss + velocity_loss + smoothness_loss + 2 * terminal_position_loss\n",
        "        total_loss = 2 * position_loss + velocity_loss + 0.5 * smoothness_loss + 2 * terminal_position_loss\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def acceleration_limit_loss(self, pred_velocity, dt=0.1):\n",
        "        # Ensure accelerations remain within feasible limits by penalizing large changes in velocity\n",
        "        approx_acceleration = (pred_velocity[:, :-1, :] - pred_velocity[:, 1:, :]) / dt  # Difference between consecutive velocities\n",
        "        acceleration_norm = torch.norm(approx_acceleration, dim=2)\n",
        "        excess_acceleration = torch.clamp(acceleration_norm - self.max_acceleration, min=0.0)\n",
        "        return torch.mean(excess_acceleration ** 2)\n",
        "\n",
        "\n",
        "    def smoothness_loss(self, pred_position):\n",
        "        # Calculate the difference between consecutive positions\n",
        "        diff = pred_position[:, :-1, :] - pred_position[:, 1:, :]\n",
        "        smoothness = torch.norm(diff, dim=2)\n",
        "        return torch.mean(smoothness)\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, initial_lr=0.001, T_max=10):\n",
        "    criterion = TrajectoryLoss()  # Use the custom loss function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)  # Adam optimizer\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        epoch_train_loss = 0  # Initialize training loss for the epoch\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            outputs = model(batch_x)  # Forward pass\n",
        "            loss = criterion(outputs, batch_y, batch_x[:, 0, :2])  # Compute loss using the custom loss function\n",
        "            loss.backward()  # Backward pass\n",
        "            # added this to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=3)\n",
        "\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            epoch_train_loss += loss.item()  # Accumulate training loss\n",
        "\n",
        "        # Update learning rate based on the cosine decay schedule\n",
        "        scheduler.step()\n",
        "\n",
        "        # Average training loss for the epoch\n",
        "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)  # Store training loss\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        epoch_val_loss = 0  # Initialize validation loss for the epoch\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "                outputs = model(batch_x)\n",
        "                loss = criterion(outputs, batch_y, batch_x[:, 0, :2])\n",
        "                epoch_val_loss += loss.item()  # Accumulate validation loss\n",
        "\n",
        "            # Average validation loss for the epoch\n",
        "            avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "            val_losses.append(avg_val_loss)  # Store validation loss\n",
        "            print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "# Create Tensor datasets\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_train.shape[2]  # Number of features\n",
        "hidden_size = 128  # You can adjust this\n",
        "# input_size = 5\n",
        "model = TrajectoryLSTM(input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1, dropout=0.5).to(\"cuda\")\n",
        "\n",
        "# Train the model with cosine decay learning rate\n",
        "train_model(model, train_loader, val_loader, num_epochs=150, initial_lr=1e-2, T_max=50)\n",
        "\n",
        "def plot_loss_curves(train_losses, val_losses):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(train_losses, label='Training Loss', color='blue')\n",
        "    plt.plot(val_losses, label='Validation Loss', color='orange')\n",
        "    plt.title('Training and Validation Loss Curves')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_curves(train_losses, val_losses)\n",
        "\n",
        "x = torch.tensor(X_val, dtype=torch.float32).to(\"cuda\")\n",
        "y = torch.tensor(y_val, dtype=torch.float32).to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(x)\n",
        "    cum_disp = torch.cumsum(outputs * 0.1, dim=1)\n",
        "    pred_position = cum_disp + y[:, 0, :2].unsqueeze(1)\n",
        "\n",
        "    distances = torch.norm(pred_position - y[:, :, :2], dim=2)\n",
        "    mean_ade = distances.mean().item()\n",
        "#LSTM\n",
        "print(\"Mean ADE:\", mean_ade)\n",
        "\n",
        "# Choose a random example from the validation dataset\n",
        "example_index = 400 # np.random.randint(0, len(X_val))\n",
        "past_traj = X_val[example_index]\n",
        "future_traj = y_val[example_index]\n",
        "\n",
        "# Get the predicted future trajectory\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  X = torch.tensor(past_traj[np.newaxis, :, :], dtype=torch.float32).to(\"cuda\")\n",
        "  predicted_vels = model(X)\n",
        "\n",
        "  predicted_vels = predicted_vels[0]\n",
        "  cum_disp = torch.cumsum(predicted_vels * 0.1, dim=1)\n",
        "  future_pred = cum_disp + X[0, -1, :2]\n",
        "  future_pred = future_pred.cpu().numpy()\n",
        "\n",
        "# print(past_traj[:, :2])\n",
        "\n",
        "\n",
        "# Plot the past trajectory, actual future trajectory, and predicted future trajectory\n",
        "#LSTM\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(past_traj[:, 0], past_traj[:, 1], label='Past Trajectory', color='blue')\n",
        "plt.plot(future_traj[:, 0], future_traj[:, 1], label='Actual Future Trajectory', color='red')\n",
        "plt.plot(future_pred[:, 0], future_pred[:, 1], label='Predicted Future Trajectory', color='green')\n",
        "plt.xlabel('X Coordinate')\n",
        "plt.ylabel('Y Coordinate')\n",
        "plt.title(f\"Predicted vs. Actual Future Trajectory (Example {example_index})\")\n",
        "plt.axis('equal')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gYMuJeom8yW"
      },
      "source": [
        "# External Actors Implementation - Positional Encoding, Dropout, Single Headed Attention\n",
        "\n",
        "Vehicle features + pedestrian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V29yLtVIm_cl",
        "outputId": "e20877a7-9632-4793-fd88-fcc6e97e6b45"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "data_path = '/content/drive/MyDrive/CS7643Group/Dataset/training/parsed_data_final_srishti.pkl'\n",
        "with open(data_path, \"rb\") as file:\n",
        "  data = pickle.load(file)\n",
        "\n",
        "print(data.shape)\n",
        "\n",
        "data = data[:, :-1, :]\n",
        "\n",
        "# This is the for all the features\n",
        "# data = data[:, :, 1:]\n",
        "\n",
        "#This is for vehicle features only\n",
        "# data = data[:, :, 1:6]\n",
        "\n",
        "# This is for vehicle + pedestrians only\n",
        "data = data[:, :, 1:8]\n",
        "\n",
        "\n",
        "# This is for vehicle + pedestrians + roadgraphs\n",
        "# another_data = data[:, :, 1:8]\n",
        "# new_data = data[:, :, 10:]\n",
        "# data = np.concatenate((another_data, new_data), axis = 2)\n",
        "\n",
        "\n",
        "print(f\"Shape of full dataset: {data.shape}\")\n",
        "# print(data[0])\n",
        "X_train = data[:int(0.8*len(data)), :80, :]\n",
        "X_val = data[int(0.8*len(data)): , :80, :]\n",
        "\n",
        "y_train = data[:int(0.8*len(data)), 80:, :]\n",
        "y_val = data[int(0.8*len(data)): , 80:, :]\n",
        "\n",
        "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
        "\n",
        "\n",
        "#Kavya and Srishti\n",
        "# External Actors added\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TrajectoryLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1, dropout=0.5):\n",
        "        super(TrajectoryLSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_features = output_features\n",
        "        self.output_timesteps = output_timesteps\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)  # Dropout layer\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Linear(hidden_size, 1)  # To compute attention scores\n",
        "        self.softmax = nn.Softmax(dim=1)  # For normalizing attention scores\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, output_features * output_timesteps)\n",
        "        )\n",
        "\n",
        "    def positional_encoding(self, seq_length, dim):\n",
        "        # Create the positional encoding matrix\n",
        "        pos = torch.arange(seq_length, dtype=torch.float32).unsqueeze(1)\n",
        "        i = torch.arange(dim, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        # Compute sinusoidal functions\n",
        "        angle_rates = 1 / (10000 ** (2 * (i // 2) / dim))\n",
        "        angle_rads = pos * angle_rates\n",
        "\n",
        "        # Apply sin to even indices and cos to odd indices\n",
        "        pos_enc = torch.zeros((seq_length, dim))\n",
        "        pos_enc[:, 0::2] = torch.sin(angle_rads[:, 0::2])  # sin for even indices\n",
        "        pos_enc[:, 1::2] = torch.cos(angle_rads[:, 1::2])  # cos for odd indices\n",
        "\n",
        "        return pos_enc\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "\n",
        "        # Add positional encoding\n",
        "        pos_enc = self.positional_encoding(seq_length, self.input_size).to(x.device)\n",
        "        x = x + pos_enc.unsqueeze(0)  # Broadcast to batch size\n",
        "\n",
        "        # LSTM forward pass\n",
        "        lstm_out, (hn, _) = self.lstm(x)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn_scores = self.attention(lstm_out)  # (batch_size, seq_length, 1)\n",
        "        attn_weights = self.softmax(attn_scores)  # Normalize to get weights\n",
        "\n",
        "        # Apply attention weights to LSTM outputs\n",
        "        weighted_output = lstm_out * attn_weights  # (batch_size, seq_length, hidden_size)\n",
        "        context_vector = torch.sum(weighted_output, dim=1)  # Sum across seq_length\n",
        "\n",
        "        # Apply dropout and fully connected layers\n",
        "        context_vector = self.dropout(context_vector)\n",
        "        out = self.fc(context_vector)  # (batch_size, output_timesteps * output_features)\n",
        "        return out.view(-1, self.output_timesteps, self.output_features)  # Reshape to (batch_size, output_timesteps, output_features)\n",
        "\n",
        "\n",
        "class TrajectoryLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TrajectoryLoss, self).__init__()\n",
        "        self.max_acceleration = 3.0\n",
        "\n",
        "    def forward(self, predictions, targets, last_input_state, dt=0.2):\n",
        "        # Reconstruct trajectory from velocities\n",
        "        pred_velocity = predictions[:, :, :]\n",
        "        cum_disp = torch.cumsum(pred_velocity * dt, dim=1)\n",
        "        # print(pred_velocity.shape)\n",
        "        # (32, 10, 2)\n",
        "\n",
        "        pred_position = cum_disp + last_input_state.unsqueeze(1)\n",
        "\n",
        "        # print(pred_position.shape)\n",
        "\n",
        "        target_position = targets[:, :, :2]\n",
        "        target_velocity = targets[:, :, 2:4]\n",
        "\n",
        "        # Calculate MSE for position\n",
        "\n",
        "        position_loss = F.mse_loss(pred_position, target_position)\n",
        "\n",
        "        velocity_loss = F.mse_loss(pred_velocity, target_velocity)\n",
        "        terminal_position_loss = F.mse_loss(pred_position[:, -1, :], target_position[:, -1, :]) + F.mse_loss(pred_position[:, 0, :], target_position[:, 0, :])\n",
        "        smoothness_loss = self.smoothness_loss(pred_position)\n",
        "\n",
        "        # total_loss = position_loss + velocity_loss + smoothness_loss + 2 * terminal_position_loss\n",
        "        total_loss = 2 * position_loss + velocity_loss + 0.5 * smoothness_loss + 2 * terminal_position_loss\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def acceleration_limit_loss(self, pred_velocity, dt=0.1):\n",
        "        # Ensure accelerations remain within feasible limits by penalizing large changes in velocity\n",
        "        approx_acceleration = (pred_velocity[:, :-1, :] - pred_velocity[:, 1:, :]) / dt  # Difference between consecutive velocities\n",
        "        acceleration_norm = torch.norm(approx_acceleration, dim=2)\n",
        "        excess_acceleration = torch.clamp(acceleration_norm - self.max_acceleration, min=0.0)\n",
        "        return torch.mean(excess_acceleration ** 2)\n",
        "\n",
        "\n",
        "    def smoothness_loss(self, pred_position):\n",
        "        # Calculate the difference between consecutive positions\n",
        "        diff = pred_position[:, :-1, :] - pred_position[:, 1:, :]\n",
        "        smoothness = torch.norm(diff, dim=2)\n",
        "        return torch.mean(smoothness)\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, initial_lr=0.001, T_max=10):\n",
        "    criterion = TrajectoryLoss()  # Use the custom loss function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)  # Adam optimizer\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        epoch_train_loss = 0  # Initialize training loss for the epoch\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            outputs = model(batch_x)  # Forward pass\n",
        "            loss = criterion(outputs, batch_y, batch_x[:, 0, :2])  # Compute loss using the custom loss function\n",
        "            loss.backward()  # Backward pass\n",
        "            # added this to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=3)\n",
        "\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            epoch_train_loss += loss.item()  # Accumulate training loss\n",
        "\n",
        "        # Update learning rate based on the cosine decay schedule\n",
        "        scheduler.step()\n",
        "\n",
        "        # Average training loss for the epoch\n",
        "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)  # Store training loss\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        epoch_val_loss = 0  # Initialize validation loss for the epoch\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")  # Move data to GPU\n",
        "\n",
        "                outputs = model(batch_x)\n",
        "                loss = criterion(outputs, batch_y, batch_x[:, 0, :2])\n",
        "                epoch_val_loss += loss.item()  # Accumulate validation loss\n",
        "\n",
        "            # Average validation loss for the epoch\n",
        "            avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "            val_losses.append(avg_val_loss)  # Store validation loss\n",
        "            print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "# Create Tensor datasets\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_train.shape[2]  # Number of features\n",
        "hidden_size = 128  # You can adjust this\n",
        "# input_size = 5\n",
        "model = TrajectoryLSTM(input_size, hidden_size, output_features=2, output_timesteps=10, num_layers=1, dropout=0.5).to(\"cuda\")\n",
        "\n",
        "# Train the model with cosine decay learning rate\n",
        "train_model(model, train_loader, val_loader, num_epochs=150, initial_lr=1e-2, T_max=50)\n",
        "\n",
        "def plot_loss_curves(train_losses, val_losses):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(train_losses, label='Training Loss', color='blue')\n",
        "    plt.plot(val_losses, label='Validation Loss', color='orange')\n",
        "    plt.title('Training and Validation Loss Curves')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_curves(train_losses, val_losses)\n",
        "\n",
        "x = torch.tensor(X_val, dtype=torch.float32).to(\"cuda\")\n",
        "y = torch.tensor(y_val, dtype=torch.float32).to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(x)\n",
        "    cum_disp = torch.cumsum(outputs * 0.1, dim=1)\n",
        "    pred_position = cum_disp + y[:, 0, :2].unsqueeze(1)\n",
        "\n",
        "    distances = torch.norm(pred_position - y[:, :, :2], dim=2)\n",
        "    mean_ade = distances.mean().item()\n",
        "#LSTM\n",
        "print(\"Mean ADE:\", mean_ade)\n",
        "\n",
        "# Choose a random example from the validation dataset\n",
        "example_index = 400 # np.random.randint(0, len(X_val))\n",
        "past_traj = X_val[example_index]\n",
        "future_traj = y_val[example_index]\n",
        "\n",
        "# Get the predicted future trajectory\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  X = torch.tensor(past_traj[np.newaxis, :, :], dtype=torch.float32).to(\"cuda\")\n",
        "  predicted_vels = model(X)\n",
        "\n",
        "  predicted_vels = predicted_vels[0]\n",
        "  cum_disp = torch.cumsum(predicted_vels * 0.1, dim=1)\n",
        "  future_pred = cum_disp + X[0, -1, :2]\n",
        "  future_pred = future_pred.cpu().numpy()\n",
        "\n",
        "# print(past_traj[:, :2])\n",
        "\n",
        "\n",
        "# Plot the past trajectory, actual future trajectory, and predicted future trajectory\n",
        "#LSTM\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(past_traj[:, 0], past_traj[:, 1], label='Past Trajectory', color='blue')\n",
        "plt.plot(future_traj[:, 0], future_traj[:, 1], label='Actual Future Trajectory', color='red')\n",
        "plt.plot(future_pred[:, 0], future_pred[:, 1], label='Predicted Future Trajectory', color='green')\n",
        "plt.xlabel('X Coordinate')\n",
        "plt.ylabel('Y Coordinate')\n",
        "plt.title(f\"Predicted vs. Actual Future Trajectory (Example {example_index})\")\n",
        "plt.axis('equal')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
